<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Policy Gradients</title>
    <link href="/2024/08/05/Policy%20Gradients/"/>
    <url>/2024/08/05/Policy%20Gradients/</url>
    
    <content type="html"><![CDATA[<h1 id="Policy-Gradients"><a href="#Policy-Gradients" class="headerlink" title="Policy Gradients"></a>Policy Gradients</h1><ul><li>对比起<code>Value-based</code>的方法(<code>Q learning, Deep Q Network</code>), Policy Gradients 直接输出动作，最大好处就是, 它能在一个连续分布上选取 action。</li></ul><h2 id="Policy-Gradients-决策"><a href="#Policy-Gradients-决策" class="headerlink" title="Policy Gradients 决策"></a>Policy Gradients 决策</h2><ul><li>行为不再是<code>Q-value</code>来选定的, 而是用<em><strong>概率</strong></em>来选定.</li></ul><h2 id="Policy-Gradients-更新"><a href="#Policy-Gradients-更新" class="headerlink" title="Policy Gradients 更新"></a>Policy Gradients 更新</h2><p><img src="/../img/blogs/rl/PG/PG02.png" alt="1720667162152"></p><ul><li>观测的信息通过神经网络分析, 选出了左边的行为, 直接进行反向传递, 使之下次被选的可能性增加, 但是奖惩信息却告诉这次的行为是不好的, 那动作可能性增加的幅度随之被减低. 这样就能靠奖励来左右神经网络反向传递。(回合更新)</li></ul><h2 id="Policy-Gradients整体算法"><a href="#Policy-Gradients整体算法" class="headerlink" title="Policy Gradients整体算法"></a>Policy Gradients整体算法</h2><p><img src="/../img/blogs/rl/PG/PG03.png" alt="1720667162152"><br><img src="/../img/blogs/rl/PG/PG04.png" alt="1720667162152"></p><ul><li><strong>吃惊度</strong>：<code>(log(Policy(s,a))*V)</code> 表示在 状态 s 对所选动作 a 的<em><strong>吃惊度</strong></em>, 如果<code>Policy(s,a)</code>概率越小, 反向的<code>log(Policy(s,a))</code>(即 -log(P)) 反而越大。如果在<code>Policy(s,a)</code>很小的情况下, 拿到了一个大的<code>R</code>, 也就是大的<code>V</code>, 那<code>-(log(Policy(s, a))*V)</code>就更大, 表示更吃惊, (我选了一个不常选的动作, 却发现原来它能得到了一个好的<code>reward</code>, 那我就得对我这次的参数进行一个大幅修改)。</li></ul>]]></content>
    
    
    <categories>
      
      <category>RL算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PPO</title>
    <link href="/2024/08/05/PPO/"/>
    <url>/2024/08/05/PPO/</url>
    
    <content type="html"><![CDATA[<h1 id="Proximal-Policy-Optimization（PPO）"><a href="#Proximal-Policy-Optimization（PPO）" class="headerlink" title="Proximal Policy Optimization（PPO）"></a>Proximal Policy Optimization（PPO）</h1><ul><li>一句话概括<code>PPO</code>: OpenAI 提出的一种解决<code>Policy Gradient</code>不好确定<code>Learning rate</code>(或者<code>Step size</code>) 的问题。 因为如果<code>step size</code>过大, 学出来的<code>Policy</code>会一直乱动, 不会收敛, 但如果<code>Step Size</code>太小, 对于完成训练, 我们会等到绝望。 <code>PPO</code>利用<code>New Policy</code>和<code>Old Policy</code>的比例, 限制了<code>New Policy</code>的更新幅度, 让<code>Policy Gradient</code>对稍微大点的<code>Step size</code>不那么敏感。</li></ul><h2 id="PPO整体算法"><a href="#PPO整体算法" class="headerlink" title="PPO整体算法"></a>PPO整体算法</h2><p><img src="/../img/blogs/rl/PPO/ppo2.png" alt="1720667078392"><br><img src="/../img/blogs/rl/PPO/ppo3.png" alt="1720667078392"></p><ul><li><code>PPO</code>是一套<code>Actor-Critic</code>结构, <code>Actor</code>想最大化<code>J_PPO</code>, <code>Critic</code>想最小化<code>L_BL</code>。<code>Critic</code>的<code>loss</code>好说, 就是减小<code>TD error</code>。而<code>Actor</code>的就是在<code>old Policy</code>上根据<code>Advantage (TD error)</code>修改<code>new Policy</code>, <code>advantage</code>大的时候, 修改幅度大, 让<code>new Policy</code>更可能发生。而且他们附加了一个<code>KL Penalty</code>(惩罚项), 简单来说, 如果<code>new Policy</code>和 <code>old Policy</code>差太多, 那<code>KL divergence</code>也越大。</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">A simple version of Proximal Policy Optimization (PPO) using single thread.</span><br><span class="hljs-string"></span><br><span class="hljs-string">Based on:</span><br><span class="hljs-string">1. Emergence of Locomotion Behaviours in Rich Environments (Google Deepmind): [https://arxiv.org/abs/1707.02286]</span><br><span class="hljs-string">2. Proximal Policy Optimization Algorithms (OpenAI): [https://arxiv.org/abs/1707.06347]</span><br><span class="hljs-string"></span><br><span class="hljs-string">View more on my tutorial website: https://morvanzhou.github.io/tutorials</span><br><span class="hljs-string"></span><br><span class="hljs-string">Dependencies:</span><br><span class="hljs-string">tensorflow r1.2</span><br><span class="hljs-string">gym 0.9.2</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> gym<br><br>EP_MAX = <span class="hljs-number">1000</span><br>EP_LEN = <span class="hljs-number">200</span><br>GAMMA = <span class="hljs-number">0.9</span><br>A_LR = <span class="hljs-number">0.0001</span><br>C_LR = <span class="hljs-number">0.0002</span><br>BATCH = <span class="hljs-number">32</span><br>A_UPDATE_STEPS = <span class="hljs-number">10</span><br>C_UPDATE_STEPS = <span class="hljs-number">10</span><br>S_DIM, A_DIM = <span class="hljs-number">3</span>, <span class="hljs-number">1</span><br>METHOD = [<br>    <span class="hljs-built_in">dict</span>(name=<span class="hljs-string">&#x27;kl_pen&#x27;</span>, kl_target=<span class="hljs-number">0.01</span>, lam=<span class="hljs-number">0.5</span>),   <span class="hljs-comment"># KL penalty</span><br>    <span class="hljs-built_in">dict</span>(name=<span class="hljs-string">&#x27;clip&#x27;</span>, epsilon=<span class="hljs-number">0.2</span>),                 <span class="hljs-comment"># Clipped surrogate objective, find this is better</span><br>][<span class="hljs-number">1</span>]        <span class="hljs-comment"># choose the method for optimization</span><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PPO</span>(<span class="hljs-title class_ inherited__">object</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.sess = tf.Session()<br>        self.tfs = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, S_DIM], <span class="hljs-string">&#x27;state&#x27;</span>)<br><br>        <span class="hljs-comment"># critic</span><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;critic&#x27;</span>):<br>            l1 = tf.layers.dense(self.tfs, <span class="hljs-number">100</span>, tf.nn.relu)<br>            self.v = tf.layers.dense(l1, <span class="hljs-number">1</span>)<br>            self.tfdc_r = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;discounted_r&#x27;</span>)<br>            self.advantage = self.tfdc_r - self.v<br>            self.closs = tf.reduce_mean(tf.square(self.advantage))<br>            self.ctrain_op = tf.train.AdamOptimizer(C_LR).minimize(self.closs)<br><br>        <span class="hljs-comment"># actor</span><br>        pi, pi_params = self._build_anet(<span class="hljs-string">&#x27;pi&#x27;</span>, trainable=<span class="hljs-literal">True</span>)<br>        oldpi, oldpi_params = self._build_anet(<span class="hljs-string">&#x27;oldpi&#x27;</span>, trainable=<span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;sample_action&#x27;</span>):<br>            self.sample_op = tf.squeeze(pi.sample(<span class="hljs-number">1</span>), axis=<span class="hljs-number">0</span>)       <span class="hljs-comment"># choosing action</span><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;update_oldpi&#x27;</span>):<br>            self.update_oldpi_op = [oldp.assign(p) <span class="hljs-keyword">for</span> p, oldp <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(pi_params, oldpi_params)]<br><br>        self.tfa = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, A_DIM], <span class="hljs-string">&#x27;action&#x27;</span>)<br>        self.tfadv = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;advantage&#x27;</span>)<br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;loss&#x27;</span>):<br>            <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;surrogate&#x27;</span>):<br>                <span class="hljs-comment"># ratio = tf.exp(pi.log_prob(self.tfa) - oldpi.log_prob(self.tfa))</span><br>                ratio = pi.prob(self.tfa) / (oldpi.prob(self.tfa) + <span class="hljs-number">1e-5</span>)<br>                surr = ratio * self.tfadv<br>            <span class="hljs-keyword">if</span> METHOD[<span class="hljs-string">&#x27;name&#x27;</span>] == <span class="hljs-string">&#x27;kl_pen&#x27;</span>:<br>                self.tflam = tf.placeholder(tf.float32, <span class="hljs-literal">None</span>, <span class="hljs-string">&#x27;lambda&#x27;</span>)<br>                kl = tf.distributions.kl_divergence(oldpi, pi)<br>                self.kl_mean = tf.reduce_mean(kl)<br>                self.aloss = -(tf.reduce_mean(surr - self.tflam * kl))<br>            <span class="hljs-keyword">else</span>:   <span class="hljs-comment"># clipping method, find this is better</span><br>                self.aloss = -tf.reduce_mean(tf.minimum(<br>                    surr,<br>                    tf.clip_by_value(ratio, <span class="hljs-number">1.</span>-METHOD[<span class="hljs-string">&#x27;epsilon&#x27;</span>], <span class="hljs-number">1.</span>+METHOD[<span class="hljs-string">&#x27;epsilon&#x27;</span>])*self.tfadv))<br><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;atrain&#x27;</span>):<br>            self.atrain_op = tf.train.AdamOptimizer(A_LR).minimize(self.aloss)<br><br>        tf.summary.FileWriter(<span class="hljs-string">&quot;log/&quot;</span>, self.sess.graph)<br><br>        self.sess.run(tf.global_variables_initializer())<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, s, a, r</span>):<br>        self.sess.run(self.update_oldpi_op)<br>        adv = self.sess.run(self.advantage, &#123;self.tfs: s, self.tfdc_r: r&#125;)<br>        <span class="hljs-comment"># adv = (adv - adv.mean())/(adv.std()+1e-6)     # sometimes helpful</span><br><br>        <span class="hljs-comment"># update actor</span><br>        <span class="hljs-keyword">if</span> METHOD[<span class="hljs-string">&#x27;name&#x27;</span>] == <span class="hljs-string">&#x27;kl_pen&#x27;</span>:<br>            <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(A_UPDATE_STEPS):<br>                _, kl = self.sess.run(<br>                    [self.atrain_op, self.kl_mean],<br>                    &#123;self.tfs: s, self.tfa: a, self.tfadv: adv, self.tflam: METHOD[<span class="hljs-string">&#x27;lam&#x27;</span>]&#125;)<br>                <span class="hljs-keyword">if</span> kl &gt; <span class="hljs-number">4</span>*METHOD[<span class="hljs-string">&#x27;kl_target&#x27;</span>]:  <span class="hljs-comment"># this in in google&#x27;s paper</span><br>                    <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">if</span> kl &lt; METHOD[<span class="hljs-string">&#x27;kl_target&#x27;</span>] / <span class="hljs-number">1.5</span>:  <span class="hljs-comment"># adaptive lambda, this is in OpenAI&#x27;s paper</span><br>                METHOD[<span class="hljs-string">&#x27;lam&#x27;</span>] /= <span class="hljs-number">2</span><br>            <span class="hljs-keyword">elif</span> kl &gt; METHOD[<span class="hljs-string">&#x27;kl_target&#x27;</span>] * <span class="hljs-number">1.5</span>:<br>                METHOD[<span class="hljs-string">&#x27;lam&#x27;</span>] *= <span class="hljs-number">2</span><br>            METHOD[<span class="hljs-string">&#x27;lam&#x27;</span>] = np.clip(METHOD[<span class="hljs-string">&#x27;lam&#x27;</span>], <span class="hljs-number">1e-4</span>, <span class="hljs-number">10</span>)    <span class="hljs-comment"># sometimes explode, this clipping is my solution</span><br>        <span class="hljs-keyword">else</span>:   <span class="hljs-comment"># clipping method, find this is better (OpenAI&#x27;s paper)</span><br>            [self.sess.run(self.atrain_op, &#123;self.tfs: s, self.tfa: a, self.tfadv: adv&#125;) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(A_UPDATE_STEPS)]<br><br>        <span class="hljs-comment"># update critic</span><br>        [self.sess.run(self.ctrain_op, &#123;self.tfs: s, self.tfdc_r: r&#125;) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(C_UPDATE_STEPS)]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_anet</span>(<span class="hljs-params">self, name, trainable</span>):<br>        <span class="hljs-keyword">with</span> tf.variable_scope(name):<br>            l1 = tf.layers.dense(self.tfs, <span class="hljs-number">100</span>, tf.nn.relu, trainable=trainable)<br>            mu = <span class="hljs-number">2</span> * tf.layers.dense(l1, A_DIM, tf.nn.tanh, trainable=trainable)<br>            sigma = tf.layers.dense(l1, A_DIM, tf.nn.softplus, trainable=trainable)<br>            norm_dist = tf.distributions.Normal(loc=mu, scale=sigma)<br>        params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)<br>        <span class="hljs-keyword">return</span> norm_dist, params<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">self, s</span>):<br>        s = s[np.newaxis, :]<br>        a = self.sess.run(self.sample_op, &#123;self.tfs: s&#125;)[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">return</span> np.clip(a, -<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_v</span>(<span class="hljs-params">self, s</span>):<br>        <span class="hljs-keyword">if</span> s.ndim &lt; <span class="hljs-number">2</span>: s = s[np.newaxis, :]<br>        <span class="hljs-keyword">return</span> self.sess.run(self.v, &#123;self.tfs: s&#125;)[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>]<br><br>env = gym.make(<span class="hljs-string">&#x27;Pendulum-v0&#x27;</span>).unwrapped<br>ppo = PPO()<br>all_ep_r = []<br><br><span class="hljs-keyword">for</span> ep <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EP_MAX):<br>    s = env.reset()<br>    buffer_s, buffer_a, buffer_r = [], [], []<br>    ep_r = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EP_LEN):    <span class="hljs-comment"># in one episode</span><br>        env.render()<br>        a = ppo.choose_action(s)<br>        s_, r, done, _ = env.step(a)<br>        buffer_s.append(s)<br>        buffer_a.append(a)<br>        buffer_r.append((r+<span class="hljs-number">8</span>)/<span class="hljs-number">8</span>)    <span class="hljs-comment"># normalize reward, find to be useful</span><br>        s = s_<br>        ep_r += r<br><br>        <span class="hljs-comment"># update ppo</span><br>        <span class="hljs-keyword">if</span> (t+<span class="hljs-number">1</span>) % BATCH == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> t == EP_LEN-<span class="hljs-number">1</span>:<br>            v_s_ = ppo.get_v(s_)<br>            discounted_r = []<br>            <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> buffer_r[::-<span class="hljs-number">1</span>]:<br>                v_s_ = r + GAMMA * v_s_<br>                discounted_r.append(v_s_)<br>            discounted_r.reverse()<br><br>            bs, ba, br = np.vstack(buffer_s), np.vstack(buffer_a), np.array(discounted_r)[:, np.newaxis]<br>            buffer_s, buffer_a, buffer_r = [], [], []<br>            ppo.update(bs, ba, br)<br>    <span class="hljs-keyword">if</span> ep == <span class="hljs-number">0</span>: all_ep_r.append(ep_r)<br>    <span class="hljs-keyword">else</span>: all_ep_r.append(all_ep_r[-<span class="hljs-number">1</span>]*<span class="hljs-number">0.9</span> + ep_r*<span class="hljs-number">0.1</span>)<br>    <span class="hljs-built_in">print</span>(<br>        <span class="hljs-string">&#x27;Ep: %i&#x27;</span> % ep,<br>        <span class="hljs-string">&quot;|Ep_r: %i&quot;</span> % ep_r,<br>        (<span class="hljs-string">&quot;|Lam: %.4f&quot;</span> % METHOD[<span class="hljs-string">&#x27;lam&#x27;</span>]) <span class="hljs-keyword">if</span> METHOD[<span class="hljs-string">&#x27;name&#x27;</span>] == <span class="hljs-string">&#x27;kl_pen&#x27;</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;&#x27;</span>,<br>    )<br><br>plt.plot(np.arange(<span class="hljs-built_in">len</span>(all_ep_r)), all_ep_r)<br>plt.xlabel(<span class="hljs-string">&#x27;Episode&#x27;</span>);plt.ylabel(<span class="hljs-string">&#x27;Moving averaged episode reward&#x27;</span>);plt.show()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>RL算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Actor Critic</title>
    <link href="/2024/08/05/Actor%20Critic/"/>
    <url>/2024/08/05/Actor%20Critic/</url>
    
    <content type="html"><![CDATA[<h1 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor Critic"></a>Actor Critic</h1><p><img src="/../img/blogs/rl/AC/AC2.png" alt="1720667162152"></p><ul><li>结合了<code>Policy Gradient (Actor)</code>和<code>Function Approximation (Critic)</code>的方法。<code>Actor</code>基于概率选行为,<code>Critic</code>基于<code>Actor</code>的行为评判行为的得分,<code>Actor</code>根据<code>Critic</code>的评分修改选行为的概率。</li></ul><h2 id="Actor-Critic优势"><a href="#Actor-Critic优势" class="headerlink" title="Actor Critic优势"></a>Actor Critic优势</h2><ul><li>可以进行单步更新, 比传统的 Policy Gradient 要快。</li></ul><h2 id="Actor-Critic劣势"><a href="#Actor-Critic劣势" class="headerlink" title="Actor Critic劣势"></a>Actor Critic劣势</h2><ul><li>取决于 Critic 的价值判断, 但是 Critic 难收敛, 再加上 Actor 的更新, 就更难收敛.</li></ul><h2 id="Actor-Critic整体算法"><a href="#Actor-Critic整体算法" class="headerlink" title="Actor Critic整体算法"></a>Actor Critic整体算法</h2><p><img src="/../img/blogs/rl/AC/AC4.png" alt="1720667162152"><br><img src="/../img/blogs/rl/AC/AC5.png" alt="1720667162152"></p><ul><li>Actor 在运用 Policy Gradient 的方法进行 Gradient ascent 的时候, 由 Critic 来告诉他, 这次的 Gradient ascent 是不是一次正确的 ascent, 如果这次的得分不好, 那么就不要 ascent 那么多。</li></ul>]]></content>
    
    
    <categories>
      
      <category>RL算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DDPG</title>
    <link href="/2024/08/05/DDPG/"/>
    <url>/2024/08/05/DDPG/</url>
    
    <content type="html"><![CDATA[<h1 id="Deep-Deterministic-Policy-Gradient（DDPG）"><a href="#Deep-Deterministic-Policy-Gradient（DDPG）" class="headerlink" title="Deep Deterministic Policy Gradient（DDPG）"></a>Deep Deterministic Policy Gradient（DDPG）</h1><p><img src="/../img/blogs/rl/DDPG/DDPG4.png" alt="1720667162152"></p><ul><li>Google DeepMind 提出的一种使用 Actor Critic 结构, 但是输出的不是行为的概率, 而是具体的行为, 用于连续动作 (continuous action) 的预测。 DDPG 结合了之前获得成功的 DQN 结构, 提高了 Actor Critic 的稳定性和收敛性。</li></ul><h2 id="DDPG更新"><a href="#DDPG更新" class="headerlink" title="DDPG更新"></a>DDPG更新</h2><ul><li><p>**<code>Actor</code>**：它的前半部分<code>grad[Q]</code>是从<code>Critic</code>来的, 这是在说: 这次<code>Actor</code>的动作要怎么移动, 才能获得更大的<code>Q</code>, 而后半部分<code>grad[μ]</code>是从<code>Actor</code>来的, 这是在说:<code>Actor</code>要怎么样修改自身参数, 使得<code>Actor</code>更有可能做这个动作。 所以两者合起来就是在说:<code>Actor</code>要朝着更有可能获取大<code>Q</code>的方向修改动作参数了。<br><img src="/../img/blogs/rl/DDPG/DDPG5.png" alt="1720667162152"></p></li><li><p>**<code>Critic</code>**：借鉴了<code>DQN</code>和<code>Double Q learning</code>的方式, 有两个计算<code>Q</code>的神经网络, <code>Q_target</code>中依据下一状态, 用<code>Actor</code>来选择动作, 而这时的<code>Actor</code>也是一个<code>Actor_target</code>(有着 Actor 很久之前的参数)。 使用这种方法获得的<code>Q_target</code>能像<code>DQN</code>那样切断相关性, 提高收敛性。<br><img src="/../img/blogs/rl/DDPG/DDPG6.png" alt="1720667162152"></p></li></ul><h2 id="DDPG整体算法"><a href="#DDPG整体算法" class="headerlink" title="DDPG整体算法"></a>DDPG整体算法</h2><p><img src="/../img/blogs/rl/DDPG/DDPG7.png" alt="1720667162152"></p><ul><li>Actor 在运用 Policy Gradient 的方法进行 Gradient ascent 的时候, 由 Critic 来告诉他, 这次的 Gradient ascent 是不是一次正确的 ascent, 如果这次的得分不好, 那么就不要 ascent 那么多。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Note: This is a updated version from my previous code,</span><br><span class="hljs-string">for the target network, I use moving average to soft replace target parameters instead using assign function.</span><br><span class="hljs-string">By doing this, it has 20% speed up on my machine (CPU).</span><br><span class="hljs-string"></span><br><span class="hljs-string">Deep Deterministic Policy Gradient (DDPG), Reinforcement Learning.</span><br><span class="hljs-string">DDPG is Actor Critic based algorithm.</span><br><span class="hljs-string">Pendulum example.</span><br><span class="hljs-string"></span><br><span class="hljs-string">View more on my tutorial page: https://morvanzhou.github.io/tutorials/</span><br><span class="hljs-string"></span><br><span class="hljs-string">Using:</span><br><span class="hljs-string">tensorflow 1.0</span><br><span class="hljs-string">gym 0.8.0</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> gym<br><span class="hljs-keyword">import</span> time<br><br><br><span class="hljs-comment">#####################  hyper parameters  ####################</span><br><br>MAX_EPISODES = <span class="hljs-number">200</span><br>MAX_EP_STEPS = <span class="hljs-number">200</span><br>LR_A = <span class="hljs-number">0.001</span>    <span class="hljs-comment"># learning rate for actor</span><br>LR_C = <span class="hljs-number">0.002</span>    <span class="hljs-comment"># learning rate for critic</span><br>GAMMA = <span class="hljs-number">0.9</span>     <span class="hljs-comment"># reward discount</span><br>TAU = <span class="hljs-number">0.01</span>      <span class="hljs-comment"># soft replacement</span><br>MEMORY_CAPACITY = <span class="hljs-number">10000</span><br>BATCH_SIZE = <span class="hljs-number">32</span><br><br>RENDER = <span class="hljs-literal">False</span><br>ENV_NAME = <span class="hljs-string">&#x27;Pendulum-v0&#x27;</span><br><br><br><span class="hljs-comment">###############################  DDPG  ####################################</span><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DDPG</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, a_dim, s_dim, a_bound,</span>):<br>        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * <span class="hljs-number">2</span> + a_dim + <span class="hljs-number">1</span>), dtype=np.float32)<br>        self.pointer = <span class="hljs-number">0</span><br>        self.sess = tf.Session()<br><br>        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound,<br>        self.S = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, s_dim], <span class="hljs-string">&#x27;s&#x27;</span>)<br>        self.S_ = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, s_dim], <span class="hljs-string">&#x27;s_&#x27;</span>)<br>        self.R = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;r&#x27;</span>)<br><br>        self.a = self._build_a(self.S,)<br>        q = self._build_c(self.S, self.a, )<br>        a_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=<span class="hljs-string">&#x27;Actor&#x27;</span>)<br>        c_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=<span class="hljs-string">&#x27;Critic&#x27;</span>)<br>        ema = tf.train.ExponentialMovingAverage(decay=<span class="hljs-number">1</span> - TAU)          <span class="hljs-comment"># soft replacement</span><br><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">ema_getter</span>(<span class="hljs-params">getter, name, *args, **kwargs</span>):<br>            <span class="hljs-keyword">return</span> ema.average(getter(name, *args, **kwargs))<br><br>        target_update = [ema.apply(a_params), ema.apply(c_params)]      <span class="hljs-comment"># soft update operation</span><br>        a_ = self._build_a(self.S_, reuse=<span class="hljs-literal">True</span>, custom_getter=ema_getter)   <span class="hljs-comment"># replaced target parameters</span><br>        q_ = self._build_c(self.S_, a_, reuse=<span class="hljs-literal">True</span>, custom_getter=ema_getter)<br><br>        a_loss = - tf.reduce_mean(q)  <span class="hljs-comment"># maximize the q</span><br>        self.atrain = tf.train.AdamOptimizer(LR_A).minimize(a_loss, var_list=a_params)<br><br>        <span class="hljs-keyword">with</span> tf.control_dependencies(target_update):    <span class="hljs-comment"># soft replacement happened at here</span><br>            q_target = self.R + GAMMA * q_<br>            td_error = tf.losses.mean_squared_error(labels=q_target, predictions=q)<br>            self.ctrain = tf.train.AdamOptimizer(LR_C).minimize(td_error, var_list=c_params)<br><br>        self.sess.run(tf.global_variables_initializer())<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">self, s</span>):<br>        <span class="hljs-keyword">return</span> self.sess.run(self.a, &#123;self.S: s[np.newaxis, :]&#125;)[<span class="hljs-number">0</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self</span>):<br>        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)<br>        bt = self.memory[indices, :]<br>        bs = bt[:, :self.s_dim]<br>        ba = bt[:, self.s_dim: self.s_dim + self.a_dim]<br>        br = bt[:, -self.s_dim - <span class="hljs-number">1</span>: -self.s_dim]<br>        bs_ = bt[:, -self.s_dim:]<br><br>        self.sess.run(self.atrain, &#123;self.S: bs&#125;)<br>        self.sess.run(self.ctrain, &#123;self.S: bs, self.a: ba, self.R: br, self.S_: bs_&#125;)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">store_transition</span>(<span class="hljs-params">self, s, a, r, s_</span>):<br>        transition = np.hstack((s, a, [r], s_))<br>        index = self.pointer % MEMORY_CAPACITY  <span class="hljs-comment"># replace the old memory with new memory</span><br>        self.memory[index, :] = transition<br>        self.pointer += <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_a</span>(<span class="hljs-params">self, s, reuse=<span class="hljs-literal">None</span>, custom_getter=<span class="hljs-literal">None</span></span>):<br>        trainable = <span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> reuse <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;Actor&#x27;</span>, reuse=reuse, custom_getter=custom_getter):<br>            net = tf.layers.dense(s, <span class="hljs-number">30</span>, activation=tf.nn.relu, name=<span class="hljs-string">&#x27;l1&#x27;</span>, trainable=trainable)<br>            a = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, name=<span class="hljs-string">&#x27;a&#x27;</span>, trainable=trainable)<br>            <span class="hljs-keyword">return</span> tf.multiply(a, self.a_bound, name=<span class="hljs-string">&#x27;scaled_a&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_c</span>(<span class="hljs-params">self, s, a, reuse=<span class="hljs-literal">None</span>, custom_getter=<span class="hljs-literal">None</span></span>):<br>        trainable = <span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> reuse <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;Critic&#x27;</span>, reuse=reuse, custom_getter=custom_getter):<br>            n_l1 = <span class="hljs-number">30</span><br>            w1_s = tf.get_variable(<span class="hljs-string">&#x27;w1_s&#x27;</span>, [self.s_dim, n_l1], trainable=trainable)<br>            w1_a = tf.get_variable(<span class="hljs-string">&#x27;w1_a&#x27;</span>, [self.a_dim, n_l1], trainable=trainable)<br>            b1 = tf.get_variable(<span class="hljs-string">&#x27;b1&#x27;</span>, [<span class="hljs-number">1</span>, n_l1], trainable=trainable)<br>            net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)<br>            <span class="hljs-keyword">return</span> tf.layers.dense(net, <span class="hljs-number">1</span>, trainable=trainable)  <span class="hljs-comment"># Q(s,a)</span><br><br><br><span class="hljs-comment">###############################  training  ####################################</span><br><br>env = gym.make(ENV_NAME)<br>env = env.unwrapped<br>env.seed(<span class="hljs-number">1</span>)<br><br>s_dim = env.observation_space.shape[<span class="hljs-number">0</span>]<br>a_dim = env.action_space.shape[<span class="hljs-number">0</span>]<br>a_bound = env.action_space.high<br><br>ddpg = DDPG(a_dim, s_dim, a_bound)<br><br>var = <span class="hljs-number">3</span>  <span class="hljs-comment"># control exploration</span><br>t1 = time.time()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(MAX_EPISODES):<br>    s = env.reset()<br>    ep_reward = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(MAX_EP_STEPS):<br>        <span class="hljs-keyword">if</span> RENDER:<br>            env.render()<br><br>        <span class="hljs-comment"># Add exploration noise</span><br>        a = ddpg.choose_action(s)<br>        a = np.clip(np.random.normal(a, var), -<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)    <span class="hljs-comment"># add randomness to action selection for exploration</span><br>        s_, r, done, info = env.step(a)<br><br>        ddpg.store_transition(s, a, r / <span class="hljs-number">10</span>, s_)<br><br>        <span class="hljs-keyword">if</span> ddpg.pointer &gt; MEMORY_CAPACITY:<br>            var *= <span class="hljs-number">.9995</span>    <span class="hljs-comment"># decay the action randomness</span><br>            ddpg.learn()<br><br>        s = s_<br>        ep_reward += r<br>        <span class="hljs-keyword">if</span> j == MAX_EP_STEPS-<span class="hljs-number">1</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Episode:&#x27;</span>, i, <span class="hljs-string">&#x27; Reward: %i&#x27;</span> % <span class="hljs-built_in">int</span>(ep_reward), <span class="hljs-string">&#x27;Explore: %.2f&#x27;</span> % var, )<br>            <span class="hljs-comment"># if ep_reward &gt; -300:RENDER = True</span><br>            <span class="hljs-keyword">break</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Running time: &#x27;</span>, time.time() - t1)<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>RL算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DQN</title>
    <link href="/2024/08/05/DQN/"/>
    <url>/2024/08/05/DQN/</url>
    
    <content type="html"><![CDATA[<h1 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h1><h2 id="神经网络的作用"><a href="#神经网络的作用" class="headerlink" title="神经网络的作用"></a>神经网络的作用</h2><p><img src="/../img/blogs/rl/DQN/DQN2.png" alt="1720667078392"></p><ul><li>方式1：将状态和动作当成神经网络的输入, 然后经过神经网络分析后得到动作的 Q 值。</li><li>方式2：只输入状态值, 输出所有的动作值, 然后按照 Q-learning 的原则, 直接选择拥有最大值的动作当做下一步要做的动作。</li></ul><h2 id="神经网络更新"><a href="#神经网络更新" class="headerlink" title="神经网络更新"></a>神经网络更新</h2><p><img src="/../img/blogs/rl/DQN/DQN3.png" alt="1720667162152"><br><img src="/../img/blogs/rl/DQN/DQN4.png" alt="1720667162152"></p><ul><li><strong>Q估计</strong>：通过<code>NN</code>预测出<code>Q(s2, a1)</code> 和<code>Q(s2,a2)</code>的值，然后选取<code>Q估计</code>中最大值的动作来换取环境中的奖励<code>reward</code>。 </li><li><strong>Q现实</strong>：通过神经网络分析下一步在<code>s&#39;</code>的两个<code>Q估计</code>值</li><li><strong>更新</strong>：神经网络的的参数就是<code>老的NN参数 + 学习率alpha × (Q现实 - Q估计)</code>。</li></ul><h2 id="DQN-两大利器"><a href="#DQN-两大利器" class="headerlink" title="DQN 两大利器"></a>DQN 两大利器</h2><p><img src="/../img/blogs/rl/DQN/DQN5.png" alt="1720667162152"><br><img src="/../img/blogs/rl/DQN/DQN6.png" alt="1720667162152"></p><ul><li><strong>Experience Replay</strong>：<code>DQN</code>有一个记忆库用于学习之前的经历，每次<code>DQN</code>更新的时候, 可以随机抽取一些之前的经历进行学习。 随机抽取这种做法打乱了经历之间的相关性, 也使得神经网络更新更有效率。(off-policy)</li><li><strong>Fixed Q-targets</strong>：在<code>DQN</code>中使用到两个结构相同但参数不同的神经网络, 预测<code>Q 估计</code>的神经网络具备最新的参数, 而预测<code>Q 现实</code>的神经网络使用的参数则是很久以前的。</li></ul><h2 id="算法伪代码"><a href="#算法伪代码" class="headerlink" title="算法伪代码"></a>算法伪代码</h2><p><img src="/../img/blogs/rl/DQN/DQN6.jpg" alt="1720667162152"></p><p>对比Q-learning：</p><ul><li>记忆库 (用于重复学习)</li><li>神经网络计算<code>Q 值</code></li><li>暂时冻结<code>q_target参数</code> (切断相关性)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">This part of code is the Q learning brain, which is a brain of the agent.</span><br><span class="hljs-string">All decisions are made in here.</span><br><span class="hljs-string"></span><br><span class="hljs-string">View more on my tutorial page: https://morvanzhou.github.io/tutorials/</span><br><span class="hljs-string"></span><br><span class="hljs-string">Pytorch: https://github.com/ClownW/Reinforcement-learning-with-PyTorch</span><br><span class="hljs-string">Tensorflow: https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><br>np.random.seed(<span class="hljs-number">1</span>)<br>tf.set_random_seed(<span class="hljs-number">1</span>)<br><br><br><span class="hljs-comment"># Deep Q Network off-policy</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DeepQNetwork</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">            self,</span><br><span class="hljs-params">            n_actions,</span><br><span class="hljs-params">            n_features,</span><br><span class="hljs-params">            learning_rate=<span class="hljs-number">0.01</span>,</span><br><span class="hljs-params">            reward_decay=<span class="hljs-number">0.9</span>,</span><br><span class="hljs-params">            e_greedy=<span class="hljs-number">0.9</span>,</span><br><span class="hljs-params">            replace_target_iter=<span class="hljs-number">300</span>,</span><br><span class="hljs-params">            memory_size=<span class="hljs-number">500</span>,</span><br><span class="hljs-params">            batch_size=<span class="hljs-number">32</span>,</span><br><span class="hljs-params">            e_greedy_increment=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">            output_graph=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    </span>):<br>        self.n_actions = n_actions<br>        self.n_features = n_features<br>        self.lr = learning_rate<br>        self.gamma = reward_decay<br>        self.epsilon_max = e_greedy<br>        self.replace_target_iter = replace_target_iter<br>        self.memory_size = memory_size<br>        self.batch_size = batch_size<br>        self.epsilon_increment = e_greedy_increment<br>        self.epsilon = <span class="hljs-number">0</span> <span class="hljs-keyword">if</span> e_greedy_increment <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> self.epsilon_max<br><br>        <span class="hljs-comment"># total learning step</span><br>        self.learn_step_counter = <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># initialize zero memory [s, a, r, s_]</span><br>        self.memory = np.zeros((self.memory_size, n_features * <span class="hljs-number">2</span> + <span class="hljs-number">2</span>))<br><br>        <span class="hljs-comment"># consist of [target_net, evaluate_net]</span><br>        self._build_net()<br><br>        t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="hljs-string">&#x27;target_net&#x27;</span>)<br>        e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="hljs-string">&#x27;eval_net&#x27;</span>)<br><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;hard_replacement&#x27;</span>):<br>            self.target_replace_op = [tf.assign(t, e) <span class="hljs-keyword">for</span> t, e <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(t_params, e_params)]<br><br>        self.sess = tf.Session()<br><br>        <span class="hljs-keyword">if</span> output_graph:<br>            <span class="hljs-comment"># $ tensorboard --logdir=logs</span><br>            tf.summary.FileWriter(<span class="hljs-string">&quot;logs/&quot;</span>, self.sess.graph)<br><br>        self.sess.run(tf.global_variables_initializer())<br>        self.cost_his = []<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_net</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># ------------------ all inputs ------------------------</span><br>        self.s = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, self.n_features], name=<span class="hljs-string">&#x27;s&#x27;</span>)  <span class="hljs-comment"># input State</span><br>        self.s_ = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, self.n_features], name=<span class="hljs-string">&#x27;s_&#x27;</span>)  <span class="hljs-comment"># input Next State</span><br>        self.r = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, ], name=<span class="hljs-string">&#x27;r&#x27;</span>)  <span class="hljs-comment"># input Reward</span><br>        self.a = tf.placeholder(tf.int32, [<span class="hljs-literal">None</span>, ], name=<span class="hljs-string">&#x27;a&#x27;</span>)  <span class="hljs-comment"># input Action</span><br><br>        w_initializer, b_initializer = tf.random_normal_initializer(<span class="hljs-number">0.</span>, <span class="hljs-number">0.3</span>), tf.constant_initializer(<span class="hljs-number">0.1</span>)<br><br>        <span class="hljs-comment"># ------------------ build evaluate_net ------------------</span><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;eval_net&#x27;</span>):<br>            e1 = tf.layers.dense(self.s, <span class="hljs-number">20</span>, tf.nn.relu, kernel_initializer=w_initializer,<br>                                 bias_initializer=b_initializer, name=<span class="hljs-string">&#x27;e1&#x27;</span>)<br>            self.q_eval = tf.layers.dense(e1, self.n_actions, kernel_initializer=w_initializer,<br>                                          bias_initializer=b_initializer, name=<span class="hljs-string">&#x27;q&#x27;</span>)<br><br>        <span class="hljs-comment"># ------------------ build target_net ------------------</span><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;target_net&#x27;</span>):<br>            t1 = tf.layers.dense(self.s_, <span class="hljs-number">20</span>, tf.nn.relu, kernel_initializer=w_initializer,<br>                                 bias_initializer=b_initializer, name=<span class="hljs-string">&#x27;t1&#x27;</span>)<br>            self.q_next = tf.layers.dense(t1, self.n_actions, kernel_initializer=w_initializer,<br>                                          bias_initializer=b_initializer, name=<span class="hljs-string">&#x27;t2&#x27;</span>)<br><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;q_target&#x27;</span>):<br>            q_target = self.r + self.gamma * tf.reduce_max(self.q_next, axis=<span class="hljs-number">1</span>, name=<span class="hljs-string">&#x27;Qmax_s_&#x27;</span>)    <span class="hljs-comment"># shape=(None, )</span><br>            self.q_target = tf.stop_gradient(q_target)<br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;q_eval&#x27;</span>):<br>            a_indices = tf.stack([tf.<span class="hljs-built_in">range</span>(tf.shape(self.a)[<span class="hljs-number">0</span>], dtype=tf.int32), self.a], axis=<span class="hljs-number">1</span>)<br>            self.q_eval_wrt_a = tf.gather_nd(params=self.q_eval, indices=a_indices)    <span class="hljs-comment"># shape=(None, )</span><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;loss&#x27;</span>):<br>            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval_wrt_a, name=<span class="hljs-string">&#x27;TD_error&#x27;</span>))<br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;train&#x27;</span>):<br>            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">store_transition</span>(<span class="hljs-params">self, s, a, r, s_</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(self, <span class="hljs-string">&#x27;memory_counter&#x27;</span>):<br>            self.memory_counter = <span class="hljs-number">0</span><br>        transition = np.hstack((s, [a, r], s_))<br>        <span class="hljs-comment"># replace the old memory with new memory</span><br>        index = self.memory_counter % self.memory_size<br>        self.memory[index, :] = transition<br>        self.memory_counter += <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">self, observation</span>):<br>        <span class="hljs-comment"># to have batch dimension when feed into tf placeholder</span><br>        observation = observation[np.newaxis, :]<br><br>        <span class="hljs-keyword">if</span> np.random.uniform() &lt; self.epsilon:<br>            <span class="hljs-comment"># forward feed the observation and get q value for every actions</span><br>            actions_value = self.sess.run(self.q_eval, feed_dict=&#123;self.s: observation&#125;)<br>            action = np.argmax(actions_value)<br>        <span class="hljs-keyword">else</span>:<br>            action = np.random.randint(<span class="hljs-number">0</span>, self.n_actions)<br>        <span class="hljs-keyword">return</span> action<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># check to replace target parameters</span><br>        <span class="hljs-keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="hljs-number">0</span>:<br>            self.sess.run(self.target_replace_op)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\ntarget_params_replaced\n&#x27;</span>)<br><br>        <span class="hljs-comment"># sample batch memory from all memory</span><br>        <span class="hljs-keyword">if</span> self.memory_counter &gt; self.memory_size:<br>            sample_index = np.random.choice(self.memory_size, size=self.batch_size)<br>        <span class="hljs-keyword">else</span>:<br>            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)<br>        batch_memory = self.memory[sample_index, :]<br><br>        _, cost = self.sess.run(<br>            [self._train_op, self.loss],<br>            feed_dict=&#123;<br>                self.s: batch_memory[:, :self.n_features],<br>                self.a: batch_memory[:, self.n_features],<br>                self.r: batch_memory[:, self.n_features + <span class="hljs-number">1</span>],<br>                self.s_: batch_memory[:, -self.n_features:],<br>            &#125;)<br><br>        self.cost_his.append(cost)<br><br>        <span class="hljs-comment"># increasing epsilon</span><br>        self.epsilon = self.epsilon + self.epsilon_increment <span class="hljs-keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="hljs-keyword">else</span> self.epsilon_max<br>        self.learn_step_counter += <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_cost</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>        plt.plot(np.arange(<span class="hljs-built_in">len</span>(self.cost_his)), self.cost_his)<br>        plt.ylabel(<span class="hljs-string">&#x27;Cost&#x27;</span>)<br>        plt.xlabel(<span class="hljs-string">&#x27;training steps&#x27;</span>)<br>        plt.show()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    DQN = DeepQNetwork(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>, output_graph=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h1 id="Double-DQN"><a href="#Double-DQN" class="headerlink" title="Double DQN"></a>Double DQN</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ul><li>在实际问题中, 如果你输出你的<code>DQN</code>的<code>Q 值</code>, 可能就会发现, <code>Q 值</code>都超级大。这是由于DQN 基于<code>Q-learning</code>, 其中<code>Qmax</code>会导致<code>Q现实</code>当中的过估计 (overestimate)。 而<code>Double DQN</code>就是用来解决过估计的。</li></ul><h2 id="Double-DQN-算法"><a href="#Double-DQN-算法" class="headerlink" title="Double DQN 算法"></a>Double DQN 算法</h2><ul><li>两个神经网络: Q_eval (Q估计中的), Q_next (Q现实中的)。</li><li>原本的<code>Q_next = max(Q_next(s&#39;, a_all))</code>。</li><li>Double DQN 中的<code>Q_next = Q_next(s&#39;, argmax(Q_eval(s&#39;, a_all)))</code>。 也可以表达成下面的式子：</li></ul><p><img src="/../img/blogs/rl/DQN/DQN7.png" alt="1720667162152"></p>]]></content>
    
    
    <categories>
      
      <category>RL算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Sarsa</title>
    <link href="/2024/07/12/Sarsa/"/>
    <url>/2024/07/12/Sarsa/</url>
    
    <content type="html"><![CDATA[<h1 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h1><h2 id="Sarsa-决策"><a href="#Sarsa-决策" class="headerlink" title="Sarsa 决策"></a>Sarsa 决策</h2><p><img src="/../img/blogs/rl/Sarsa/s2.png" alt="1720667078392"></p><ul><li>学习完成后，根据当前状态在Q值表中的<em><strong>最大</strong></em>Q值来选取动作</li></ul><h2 id="Sarsa更新"><a href="#Sarsa更新" class="headerlink" title="Sarsa更新"></a>Sarsa更新</h2><p><img src="/../img/blogs/rl/Sarsa/s3.png" alt="1720667162152"></p><ul><li><strong>更新Q值表</strong>：通过计算现实Q值和估计Q值的差距来更新</li><li><strong>现实Q值</strong>：估算的动作也是接下来要做的动作（on-policy）</li><li><strong>估计Q值</strong>：原Q值表中对应的Q值</li></ul><h2 id="Sarsa整体算法"><a href="#Sarsa整体算法" class="headerlink" title="Sarsa整体算法"></a>Sarsa整体算法</h2><p><img src="/../img/blogs/rl/Sarsa/s4.png" alt="1720667162152"></p><ul><li><strong>不同之处</strong>：Sarsa是说到做到型，所以称为on-policy，在线学习，学着自己在做的事情。而Q-learning 是说到但并不一定做到，所以称为Off-policy，离线学习。</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">This part of code is the Q learning brain, which is a brain of the agent.</span><br><span class="hljs-string">All decisions are made in here.</span><br><span class="hljs-string"></span><br><span class="hljs-string">View more on my tutorial page: https://morvanzhou.github.io/tutorials/</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RL</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, action_space, learning_rate=<span class="hljs-number">0.01</span>, reward_decay=<span class="hljs-number">0.9</span>, e_greedy=<span class="hljs-number">0.9</span></span>):<br>        self.actions = action_space  <span class="hljs-comment"># a list</span><br>        self.lr = learning_rate<br>        self.gamma = reward_decay<br>        self.epsilon = e_greedy<br><br>        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">check_state_exist</span>(<span class="hljs-params">self, state</span>):<br>        <span class="hljs-keyword">if</span> state <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.q_table.index:<br>            <span class="hljs-comment"># append new state to q table</span><br>            self.q_table = self.q_table.append(<br>                pd.Series(<br>                    [<span class="hljs-number">0</span>]*<span class="hljs-built_in">len</span>(self.actions),<br>                    index=self.q_table.columns,<br>                    name=state,<br>                )<br>            )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">self, observation</span>):<br>        self.check_state_exist(observation)<br>        <span class="hljs-comment"># action selection</span><br>        <span class="hljs-keyword">if</span> np.random.rand() &lt; self.epsilon:<br>            <span class="hljs-comment"># choose best action</span><br>            state_action = self.q_table.loc[observation, :]<br>            <span class="hljs-comment"># some actions may have the same value, randomly choose on in these actions</span><br>            action = np.random.choice(state_action[state_action == np.<span class="hljs-built_in">max</span>(state_action)].index)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># choose random action</span><br>            action = np.random.choice(self.actions)<br>        <span class="hljs-keyword">return</span> action<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self, *args</span>):<br>        <span class="hljs-keyword">pass</span><br><br><br><span class="hljs-comment"># off-policy</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">QLearningTable</span>(<span class="hljs-title class_ inherited__">RL</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, actions, learning_rate=<span class="hljs-number">0.01</span>, reward_decay=<span class="hljs-number">0.9</span>, e_greedy=<span class="hljs-number">0.9</span></span>):<br>        <span class="hljs-built_in">super</span>(QLearningTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self, s, a, r, s_</span>):<br>        self.check_state_exist(s_)<br>        q_predict = self.q_table.loc[s, a]<br>        <span class="hljs-keyword">if</span> s_ != <span class="hljs-string">&#x27;terminal&#x27;</span>:<br>            q_target = r + self.gamma * self.q_table.loc[s_, :].<span class="hljs-built_in">max</span>()  <span class="hljs-comment"># next state is not terminal</span><br>        <span class="hljs-keyword">else</span>:<br>            q_target = r  <span class="hljs-comment"># next state is terminal</span><br>        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  <span class="hljs-comment"># update</span><br><br><br><span class="hljs-comment"># on-policy</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SarsaTable</span>(<span class="hljs-title class_ inherited__">RL</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, actions, learning_rate=<span class="hljs-number">0.01</span>, reward_decay=<span class="hljs-number">0.9</span>, e_greedy=<span class="hljs-number">0.9</span></span>):<br>        <span class="hljs-built_in">super</span>(SarsaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self, s, a, r, s_, a_</span>):<br>        self.check_state_exist(s_)<br>        q_predict = self.q_table.loc[s, a]<br>        <span class="hljs-keyword">if</span> s_ != <span class="hljs-string">&#x27;terminal&#x27;</span>:<br>            q_target = r + self.gamma * self.q_table.loc[s_, a_]  <span class="hljs-comment"># next state is not terminal</span><br>        <span class="hljs-keyword">else</span>:<br>            q_target = r  <span class="hljs-comment"># next state is terminal</span><br>        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  <span class="hljs-comment"># update</span><br></code></pre></td></tr></table></figure><h1 id="Sarsa-λ"><a href="#Sarsa-λ" class="headerlink" title="Sarsa(λ)"></a>Sarsa(λ)</h1><h2 id="单步更新-and-回合更新"><a href="#单步更新-and-回合更新" class="headerlink" title="单步更新 and 回合更新"></a>单步更新 and 回合更新</h2><p><img src="/../img/blogs/rl/Sarsa/sl2.png" alt="1720667162152"></p><h2 id="λ取值"><a href="#λ取值" class="headerlink" title="λ取值"></a>λ取值</h2><p><img src="/../img/blogs/rl/Sarsa/sl3.png" alt="1720667162152"></p><ul><li>当<code>λ=0</code>, 就变成了Sarsa的单步更新, 当<code>λ=1</code>, 就变成了回合更新, 对所有步更新的力度都是一样。 当<code>λ=(0,1)</code>之间, 取值越大, 离宝藏越近的步更新力度越大。</li></ul><h2 id="Sarsa-λ-整体算法"><a href="#Sarsa-λ-整体算法" class="headerlink" title="Sarsa(λ)整体算法"></a>Sarsa(λ)整体算法</h2><p><img src="/../img/blogs/rl/Sarsa/sl4.png" alt="sl4"></p><h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">This part of code is the Q learning brain, which is a brain of the agent.</span><br><span class="hljs-string">All decisions are made in here.</span><br><span class="hljs-string"></span><br><span class="hljs-string">View more on my tutorial page: https://morvanzhou.github.io/tutorials/</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RL</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, action_space, learning_rate=<span class="hljs-number">0.01</span>, reward_decay=<span class="hljs-number">0.9</span>, e_greedy=<span class="hljs-number">0.9</span></span>):<br>        self.actions = action_space  <span class="hljs-comment"># a list</span><br>        self.lr = learning_rate<br>        self.gamma = reward_decay<br>        self.epsilon = e_greedy<br><br>        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">check_state_exist</span>(<span class="hljs-params">self, state</span>):<br>        <span class="hljs-keyword">if</span> state <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.q_table.index:<br>            <span class="hljs-comment"># append new state to q table</span><br>            self.q_table = self.q_table.append(<br>                pd.Series(<br>                    [<span class="hljs-number">0</span>]*<span class="hljs-built_in">len</span>(self.actions),<br>                    index=self.q_table.columns,<br>                    name=state,<br>                )<br>            )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">self, observation</span>):<br>        self.check_state_exist(observation)<br>        <span class="hljs-comment"># action selection</span><br>        <span class="hljs-keyword">if</span> np.random.rand() &lt; self.epsilon:<br>            <span class="hljs-comment"># choose best action</span><br>            state_action = self.q_table.loc[observation, :]<br>            <span class="hljs-comment"># some actions may have the same value, randomly choose on in these actions</span><br>            action = np.random.choice(state_action[state_action == np.<span class="hljs-built_in">max</span>(state_action)].index)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># choose random action</span><br>            action = np.random.choice(self.actions)<br>        <span class="hljs-keyword">return</span> action<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self, *args</span>):<br>        <span class="hljs-keyword">pass</span><br><br><br><span class="hljs-comment"># backward eligibility traces</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SarsaLambdaTable</span>(<span class="hljs-title class_ inherited__">RL</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, actions, learning_rate=<span class="hljs-number">0.01</span>, reward_decay=<span class="hljs-number">0.9</span>, e_greedy=<span class="hljs-number">0.9</span>, trace_decay=<span class="hljs-number">0.9</span></span>):<br>        <span class="hljs-built_in">super</span>(SarsaLambdaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)<br><br>        <span class="hljs-comment"># backward view, eligibility trace.</span><br>        self.lambda_ = trace_decay<br>        self.eligibility_trace = self.q_table.copy()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">check_state_exist</span>(<span class="hljs-params">self, state</span>):<br>        <span class="hljs-keyword">if</span> state <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.q_table.index:<br>            <span class="hljs-comment"># append new state to q table</span><br>            to_be_append = pd.Series(<br>                    [<span class="hljs-number">0</span>] * <span class="hljs-built_in">len</span>(self.actions),<br>                    index=self.q_table.columns,<br>                    name=state,<br>                )<br>            self.q_table = self.q_table.append(to_be_append)<br><br>            <span class="hljs-comment"># also update eligibility trace</span><br>            self.eligibility_trace = self.eligibility_trace.append(to_be_append)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self, s, a, r, s_, a_</span>):<br>        self.check_state_exist(s_)<br>        q_predict = self.q_table.loc[s, a]<br>        <span class="hljs-keyword">if</span> s_ != <span class="hljs-string">&#x27;terminal&#x27;</span>:<br>            q_target = r + self.gamma * self.q_table.loc[s_, a_]  <span class="hljs-comment"># next state is not terminal</span><br>        <span class="hljs-keyword">else</span>:<br>            q_target = r  <span class="hljs-comment"># next state is terminal</span><br>        error = q_target - q_predict<br><br>        <span class="hljs-comment"># increase trace amount for visited state-action pair</span><br><br>        <span class="hljs-comment"># Method 1:</span><br>        <span class="hljs-comment"># self.eligibility_trace.loc[s, a] += 1</span><br><br>        <span class="hljs-comment"># Method 2:</span><br>        self.eligibility_trace.loc[s, :] *= <span class="hljs-number">0</span><br>        self.eligibility_trace.loc[s, a] = <span class="hljs-number">1</span><br><br>        <span class="hljs-comment"># Q update</span><br>        self.q_table += self.lr * error * self.eligibility_trace<br><br>        <span class="hljs-comment"># decay eligibility trace after update</span><br>        self.eligibility_trace *= self.gamma*self.lambda_<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>RL算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Q-Learning</title>
    <link href="/2024/07/12/Q-Learning/"/>
    <url>/2024/07/12/Q-Learning/</url>
    
    <content type="html"><![CDATA[<h1 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h1><h2 id="QLearning-决策"><a href="#QLearning-决策" class="headerlink" title="QLearning 决策"></a>QLearning 决策</h2><p><img src="/../img/blogs/rl/Q-Learning/q2.png" alt="1720667078392"></p><ul><li>学习完成后，根据当前状态在Q值表中的<em><strong>最大</strong></em>Q值来选取动作</li></ul><h2 id="QLearning更新"><a href="#QLearning更新" class="headerlink" title="QLearning更新"></a>QLearning更新</h2><p><img src="/../img/blogs/rl/Q-Learning/q3.png" alt="1720667162152"></p><ul><li><strong>更新Q值表</strong>：通过计算现实Q值和估计Q值的差距来更新</li><li><strong>现实Q值</strong>：通过<em><strong>想象</strong></em>在下个状态选择的Q值（<em><strong>max</strong></em>）乘上衰减系数，并加上到达下个状态的奖励作为现实Q值（off-policy）</li><li><strong>估计Q值</strong>：原Q值表中对应的Q值</li></ul><h2 id="QLearning整体算法"><a href="#QLearning整体算法" class="headerlink" title="QLearning整体算法"></a>QLearning整体算法</h2><p><img src="/../img/blogs/rl/Q-Learning/q4.png" alt="1720667162152"></p><ul><li><strong>迷人之处</strong>：在<code>Q(s1, a2)</code>现实中, 也包含了一个<code>Q(s2)</code>的最大估计值，将对下一步的衰减的最大估计和当前所得到的奖励当成这一步的现实Q值。</li><li><strong>Epsilon greedy</strong>：是用在决策上的一种策略, 比如 epsilon &#x3D; 0.9 时, 就说明有90% 的情况我会按照 Q 表的最优值选择行为, 10% 的时间使用随机选行为。</li><li><strong>alpha</strong>：学习率, 来决定这次的误差有多少是要被学习的, alpha是一个小于1 的数。</li><li><strong>gamma</strong>：对未来 reward 的衰减值。</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">This part of code is the Q learning brain, which is a brain of the agent.</span><br><span class="hljs-string">All decisions are made in here.</span><br><span class="hljs-string"></span><br><span class="hljs-string">View more on my tutorial page: https://morvanzhou.github.io/tutorials/</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>np.random.seed(<span class="hljs-number">2</span>)  <span class="hljs-comment"># reproducible</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">QLearningTable</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, actions, learning_rate=<span class="hljs-number">0.01</span>, reward_decay=<span class="hljs-number">0.9</span>, e_greedy=<span class="hljs-number">0.9</span></span>):<br>        self.actions = actions  <span class="hljs-comment"># a list</span><br>        self.lr = learning_rate<br>        self.gamma = reward_decay<br>        self.epsilon = e_greedy<br>        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">self, observation</span>):<br>        self.check_state_exist(observation)<br>        <span class="hljs-comment"># action selection</span><br>        <span class="hljs-keyword">if</span> np.random.uniform() &lt; self.epsilon:<br>            <span class="hljs-comment"># choose best action</span><br>            state_action = self.q_table.loc[observation, :]<br>            <span class="hljs-comment"># some actions may have the same value, randomly choose on in these actions</span><br>            action = np.random.choice(state_action[state_action == np.<span class="hljs-built_in">max</span>(state_action)].index)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># choose random action</span><br>            action = np.random.choice(self.actions)<br>        <span class="hljs-keyword">return</span> action<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self, s, a, r, s_</span>):<br>        self.check_state_exist(s_)<br>        q_predict = self.q_table.loc[s, a]<br>        <span class="hljs-keyword">if</span> s_ != <span class="hljs-string">&#x27;terminal&#x27;</span>:<br>            q_target = r + self.gamma * self.q_table.loc[s_, :].<span class="hljs-built_in">max</span>()  <span class="hljs-comment"># next state is not terminal</span><br>        <span class="hljs-keyword">else</span>:<br>            q_target = r  <span class="hljs-comment"># next state is terminal</span><br>        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  <span class="hljs-comment"># update</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">check_state_exist</span>(<span class="hljs-params">self, state</span>):<br>        <span class="hljs-keyword">if</span> state <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.q_table.index:<br>            <span class="hljs-comment"># append new state to q table</span><br>            self.q_table = self.q_table.append(<br>                pd.Series(<br>                    [<span class="hljs-number">0</span>]*<span class="hljs-built_in">len</span>(self.actions),<br>                    index=self.q_table.columns,<br>                    name=state,<br>                )<br>            )<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>RL算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker 学习</title>
    <link href="/2024/05/26/docker_learn_note/"/>
    <url>/2024/05/26/docker_learn_note/</url>
    
    <content type="html"><![CDATA[<h2 id="docker-学习笔记"><a href="#docker-学习笔记" class="headerlink" title="docker 学习笔记"></a>docker 学习笔记</h2><h3 id="1-基础教程"><a href="#1-基础教程" class="headerlink" title="1. 基础教程"></a>1. 基础教程</h3><p>参考教程：</p><ol><li><a href="https://blog.csdn.net/qq_40298902/article/details/105967342">菜鸟入门Docker</a></li><li><a href="https://blog.csdn.net/qq_32101863/article/details/120341856">Docker 封装anaconda环境，生成镜像并打包1</a></li><li><a href="https://blog.csdn.net/qq_32101863/article/details/120344080">Docker 封装anaconda环境，生成镜像并打包2</a></li><li><a href="https://www.runoob.com/docker/docker-image-usage.html">Docker 菜鸟教程</a></li></ol><h3 id="2-docker镜像准备"><a href="#2-docker镜像准备" class="headerlink" title="2. docker镜像准备"></a>2. docker镜像准备</h3><h4 id="下载基础镜像"><a href="#下载基础镜像" class="headerlink" title="下载基础镜像"></a>下载基础镜像</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker pull ubuntu:18.04<br></code></pre></td></tr></table></figure><h4 id="制作镜像"><a href="#制作镜像" class="headerlink" title="制作镜像"></a>制作镜像</h4><p>编写Dockerfile文件，内容示例如下：<br>    <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">使用官方的 Python 3.8 镜像作为基础镜像</span><br>FROM python:3.8<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">安装 wget</span><br><br>RUN apt-get update &amp;&amp; apt-get install -y wget &amp;&amp; rm -rf /var/lib/apt/lists/*<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">下载 Miniconda 安装脚本</span><br><br>RUN wget -q[https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh](https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh) -O miniconda.sh<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">安装 Miniconda</span><br><br>RUN bash miniconda.sh -b -p /opt/miniconda &amp;&amp;<br>rm miniconda.sh<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">初始化 Conda</span><br><br>RUN /opt/miniconda/bin/conda init bash<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">创建新的 Conda 环境</span><br><br>ENV CONDA_ENV=myenv<br>RUN conda create -y --name $CONDA_ENV python=3.8<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">激活 Conda 环境</span><br><br>RUN echo &quot;conda activate $CONDA_ENV&quot; &gt;&gt; ~/.bashrc<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">安装所需的包</span><br><br>RUN conda install -y -n $CONDA_ENV numpy pandas matplotlib<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">设置容器内的工作目录</span><br><br>WORKDIR /usr/src/app<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">设置容器启动时的命令</span><br><br>CMD [&quot;/bin/bash&quot;]<br></code></pre></td></tr></table></figure></p><h4 id="构建镜像"><a href="#构建镜像" class="headerlink" title="构建镜像"></a>构建镜像</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker build --network host -t mirrors.tencent.com/star_library/tlinux-64bit-python3.6:0415 .<br></code></pre></td></tr></table></figure><h4 id="推送镜像"><a href="#推送镜像" class="headerlink" title="推送镜像"></a>推送镜像</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">以下方括号内容为需要根据实际需求替换的字符串</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">登录至软件源docker registry，[user]为OA账号名，[token]为软件源系统分配，可以通过【快捷指令】获取</span><br>sudo docker login --username [user] --password [token] mirrors.tencent.com<br><span class="hljs-meta prompt_"> </span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">push镜像至软件源docker registry，[image_id]即docker镜像的<span class="hljs-built_in">id</span>，可以通过sudo docker image list命令查看</span><br>sudo docker tag [image_id] mirrors.tencent.com/[namespace]/[repo]:[tag]<br>sudo docker push mirrors.tencent.com/[namespace]/[repo]:[tag]<br><br>如<br>docker push mirrors.tencent.com/gethinhu/tlinux-64bit-python3.6:0415<br></code></pre></td></tr></table></figure><h3 id="3-调试"><a href="#3-调试" class="headerlink" title="3. 调试"></a>3. 调试</h3><p>为了保证调试时训练程序运行环境与线上实际的运行环境一致，需要在开发机上用与线上相同的docker镜像启动容器</p><p>为了方便，平台也准备了一个脚本run_docker.sh，可以从附件下载使用。使用方法：</p><p>注意：</p><p>如果报错‘run_docker.sh : command not found’，请检查&#x2F;usr&#x2F;local&#x2F;bin是否包含在$PATH里，如果没有，将其加入$PATH。</p><p>如果报错‘docker: Error response from daemon: could not select device driver “” with capabilities: [[gpu]].’，是因为开发机没安装nvidia-docker，可以联系O2000(Oteam统一技术支持)解决。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">1. 编写run_docker.sh</span><br>        # 示例<br><br>        #!/bin/bash<br>        image_full_name=$1<br>        if [ -z $&#123;image_full_name&#125; ]<br>        then<br>        echo &quot;mirrors.tencent.com/lllzheng/cuda11.8-ubuntu20.04:0511    \n&quot;<br>        exit 0<br>        fi<br><br>        cmd=$2<br>        if [ -z &quot;$&#123;cmd&#125;&quot; ]<br>        then<br>        echo &quot;cd /apdcephfs/private_lllzheng/taiji/peg-in-hole-6D/     \n&quot;<br>        echo &quot;conda activate mujoco     \n&quot;<br>        echo &quot;python3 /apdcephfs/private_lllzheng/taiji/peg-in-hole-6D/try_train.py     \n&quot;<br>        exit 0<br>        fi<br><br>        echo $&#123;image_full_name&#125;<br>        echo $&#123;cmd&#125;<br>        docker run -it --gpus all --network=host --name test -v /apdcephfs/:/apdcephfs/private_lllzheng  $&#123;image_full_name&#125;  $&#123;cmd&#125;<br><br>        #如果docker -v版本小于19.03，请使用以下docker run命令<br>        #docker run -it -e NVIDIA_VISIBLE_DEVICES=all --network=host -v /apdcephfs/:/apdcephfs/  $&#123;image_full_name&#125;  $&#123;cmd&#125;<br><br>        # sudo bash run_docker.sh mirrors.tencent.com/lllzheng/cuda11.8-ubuntu20.04:0511 /bin/bash -c &quot;cd /apdcephfs/private_lllzheng/taiji/peg-in-hole-6D/ &amp;&amp; conda activate mujoco &amp;&amp; python3 /apdcephfs/private_lllzheng/taiji/peg-in-hole-6D/try_train.py&quot;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">2. 运行run_docker.sh</span><br>chmod +x run_docker.sh<br>run_docker.sh $&#123;image_full_name&#125; $&#123;cmd&#125;<br>eg：run_docker.sh mirrors.tencent.com/star_library/g-tlinux2.2-python3.6-cuda9.0-cudnn7.6-tf1.12:latest /bin/bash -c &quot;python3.6 /apdcephfs/private_gethinhu/mnist_trainer/fully_connected_feed.py --train_dir /apdcephfs/share_986015/mnist_dataset/ --max_steps 100000 --batch_size=1000&quot;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mujoco-py 学习</title>
    <link href="/2024/05/26/mujoco-py%20learn_note/"/>
    <url>/2024/05/26/mujoco-py%20learn_note/</url>
    
    <content type="html"><![CDATA[<h2 id="mujoco-py-学习笔记"><a href="#mujoco-py-学习笔记" class="headerlink" title="mujoco-py 学习笔记"></a>mujoco-py 学习笔记</h2><h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install mujoco-py<br></code></pre></td></tr></table></figure><h3 id="常见安装错误"><a href="#常见安装错误" class="headerlink" title="常见安装错误"></a>常见安装错误</h3><ul><li>如果报错 Cython.Compiler.Errors.CompileError:</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip uninstall cython<br>pip install cython==0.29.21<br></code></pre></td></tr></table></figure><ul><li>tensorboard打不开</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 第一步：查看端口占用：</span><br>lsof -i:6006<br><span class="hljs-comment"># 第二步：关掉占用端口的进程</span><br><span class="hljs-built_in">kill</span> -9 PID<br></code></pre></td></tr></table></figure><h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p><a href="https://blog.csdn.net/weixin_44420419/article/details/116231500">mujoco-py安装遇问题</a><br><a href="https://blog.csdn.net/weixin_44420419/article/details/116519279">mujoco-py渲染问题</a></p><h3 id="小记"><a href="#小记" class="headerlink" title="小记"></a>小记</h3><h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> mujoco_py<br>model = mujoco_py.load_model_from_path(<span class="hljs-string">&quot;path/to/model.xml&quot;</span>)<br>sim = mujoco_py.MjSim(model)<br></code></pre></td></tr></table></figure><p>创建了一个 MjSim 对象，它表示一个 MuJoCo 仿真实例。通过这个对象，执行各种操作，例如执行仿真步骤、获取和设置模型状态、渲染仿真环境等。使用 sim 对象实现或获取的操作：</p><ul><li>执行仿真步骤：使用 <code>sim.step()</code>方法，执行一步仿真。这将根据当前的模型状态和控制输入更新模型的状态。</li><li>获取和设置模型状态：使用 <code>sim.data</code>属性来获取和设置模型的状态。例如，获取关节角度、速度、力矩等信息，也可以设置关节力矩、外部力等。</li><li>渲染仿真环境：使用 <code>sim.render()</code>方法，渲染仿真环境。这将在屏幕上显示仿真环境的当前状态。</li><li>获取和设置控制输入：使用 <code>sim.data.ctrl</code>属性来获取和设置控制输入。例如，设置关节的目标位置、速度或力矩等。</li><li>获取和设置模型参数：使用 <code>sim.model</code>属性来获取和设置模型参数。例如，获取关节的惯量、刚度等信息，也可以设置关节的摩擦系数、阻尼系数等。</li><li>保存和加载仿真状态：使用 <code>sim.save()</code>和 <code>sim.load()</code>方法来保存和加载仿真状态。这可以用于暂停和恢复仿真，或者在多个仿真实例之间共享状态。</li><li>获取和设置相机参数：使用 <code>sim.render_context</code>属性来获取和设置相机参数。例如，设置相机的位置、方向、视野角等。</li></ul><h5 id="sim-data中一些主要的属性"><a href="#sim-data中一些主要的属性" class="headerlink" title="sim.data中一些主要的属性"></a>sim.data中一些主要的属性</h5><ul><li>qpos：关节位置的数组，这个数组包含了模拟中所有关节的位置和所有自由物体的位置和方向。</li><li>qvel：关节速度的数组。</li><li>qacc：关节加速度的数组。</li><li>qfrc_applied：施加到每个关节的外部力的数组。</li><li>qfrc_actuator:</li><li>ctrl：控制器的控制信号的数组。</li><li>time：模拟的当前时间。</li><li>xpos：模型中每个物体的位置的数组。</li><li>xquat：模型中每个物体的四元数表示的方向的数组。</li><li>xmat：模型中每个物体的旋转矩阵表示的方向的数组。</li><li>xipos：模型中每个惯性元素的位置的数组。</li><li>ximat：模型中每个惯性元素的旋转矩阵表示的方向的数组。</li><li>contact：模型中的接触信息。</li></ul><h5 id="PyMjData"><a href="#PyMjData" class="headerlink" title="PyMjData"></a>PyMjData</h5><ul><li><p><code>act</code>: 执行器激活数组，用于控制执行器的激活状态。</p></li><li><p><code>act_dot</code>: 执行器激活的变化率。</p></li><li><p><code>active_contacts_efc_pos</code>: 活动接触点的全局位置。</p></li><li><p><code>actuator_force</code>: 执行器产生的力。</p></li><li><p><code>actuator_length</code>: 执行器的长度。</p></li><li><p><code>actuator_moment</code>: 执行器产生的力矩。</p></li><li><p><code>actuator_velocity</code>: 执行器的速度。</p></li><li><p><code>body_jacp, body_jacr</code>: 与身体相关的雅可比矩阵的偏导数。</p></li><li><p><code>body_xmat, body_xpos, body_xquat, body_xvelp, body_xvelr</code>: 描述身体的位置、方向、速度和旋转的矩阵和向量。</p></li><li><p><code>cacc</code>: 接触点的累积加速度。</p></li><li><p><code>cam_xmat, cam_xpos</code>: 相机的位置和方向。</p></li><li><p><code>cdof, cdof_dot</code>: 自由度的当前值和变化率。</p></li><li><p><code>cfrc_ext</code>: 外部力。</p></li><li><p><code>cfrc_int</code>: 内部力。</p></li><li><p><code>cinert</code>: 惯性张量。</p></li><li><p><code>contact</code>: 接触信息。</p></li><li><p><code>crb</code>: 约束雅可比矩阵。</p></li><li><p><code>ctrl</code>: 控制输入。</p></li><li><p><code>cvel</code>: 接触点的速度。</p></li><li><p><code>efc_AR, efc_D, efc_J, efc_R</code>: 接触力雅可比矩阵、阻尼矩阵、约束雅可比矩阵和旋转矩阵。</p></li><li><p><code>energy</code>: 系统能量。</p></li><li><p><code>geom_jacp, geom_jacr, geom_xmat, geom_xpos, geom_xvelp, geom_xvelr</code>: 几何体的位置、方向、速度。</p></li><li><p><code>light_xdir, light_xpos</code>: 光源的方向和位置。</p></li><li><p><code>maxuse_con, maxuse_efc, maxuse_stack</code>: 各种数组的最大使用数量。</p></li><li><p><code>mocap_pos, mocap_quat</code>: 运动捕捉的位置和四元数。</p></li><li><p><code>nbuffer, ncon, ne, nefc, nf, nstack</code>: 不同类型数据的数量。</p></li><li><p><code>pstack</code>: 预测栈。</p></li><li><p><code>qLD, qLDiagInv, qLDiagSqrtInvInv, qM, qacc, qacc_unc, qacc_warmstart</code>: 与线性代数相关的矩阵和向量。</p></li><li><p><code>qfrc_actuator, qfrc_applied, qfrc_bias, qfrc_constraint, qfrc_inverse, qfrc_passive, qfrc_unc</code>: 与力相关的向量。</p></li><li><p><code>qpos, qvel</code>: 关节位置和速度状态。</p></li><li><p><code>sensordata</code>: 传感器数据。</p></li><li><p><code>set_joint_qpos, set_joint_qvel</code>: 设置关节位置和速度的方法。</p></li><li><p><code>set_mocap_pos, set_mocap_quat</code>: 设置运动捕捉位置和四元数的方法。</p></li><li><p><code>site_jacp, site_jacr, site_xmat, site_xpos, site_xvelp, site_xvelr</code>: 站点的雅可比、位置、方向、速度。</p></li><li><p><code>solver, solver_fwdinv, solver_iter, solver_nnz</code>: 与求解器相关的属性。</p></li><li><p><code>subtree_angmom, subtree_com, subtree_linvel</code>: 子树的角动量、质心和线性速度。</p></li><li><p><code>ten_length, ten_moment, ten_velocity, ten_wrapadr, ten_wrapnum</code>: 肌腱的长度、力矩、速度和缠绕信息。</p></li><li><p><code>time, timer</code>: 仿真时间。</p></li><li><p><code>userdata</code>: 用户自定义数据。</p></li><li><p><code>warning</code>: 警告信息。</p></li><li><p><code>wrap_obj, wrap_xpos</code>: 与肌腱缠绕相关的对象和位置。</p></li><li><p><code>xanchor, xaxis</code>: 与接触有关的锚点和轴。</p></li><li><p><code>ximat, xpos</code>: 位置和方向。</p></li><li><p><code>xfrc_applied</code>: 应用的外部力。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>Mujoco</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Mujoco</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mujoco Q&amp;A</title>
    <link href="/2024/05/25/mujoco_question/"/>
    <url>/2024/05/25/mujoco_question/</url>
    
    <content type="html"><![CDATA[<h2 id="一、Mujoco安装"><a href="#一、Mujoco安装" class="headerlink" title="一、Mujoco安装"></a>一、Mujoco安装</h2><p>这是一句话<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Mujoco Python](https://mujoco.readthedocs.io/en/stable/python.html#)">[1]</span></a></sup></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://mujoco.readthedocs.io/en/stable/python.html#">Mujoco Python</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Mujoco</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Mujoco</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Uubuntu20.04 系统配置</title>
    <link href="/2024/05/25/ubuntu%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE/"/>
    <url>/2024/05/25/ubuntu%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="一、-安装浏览器"><a href="#一、-安装浏览器" class="headerlink" title="一、 安装浏览器"></a>一、 安装浏览器</h2><ul><li><p>安装Edge浏览器</p></li><li><p>安装网络工具<br>工具在手，天下我有！<br><a href="https://github.com/lantongxue/clash_for_windows_pkg/releases">下载地址</a></p></li><li><p>VsCode安装和更新</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 安装</span><br>wget http://fishros.com/install -O fishros &amp;&amp; . fishros<br><br><span class="hljs-comment">#先更新源</span><br>sudo apt-get update<br><br><span class="hljs-comment">#再安装vscode</span><br>sudo apt-get install code<br></code></pre></td></tr></table></figure></li></ul><h2 id="二、配置终端"><a href="#二、配置终端" class="headerlink" title="二、配置终端"></a>二、配置终端</h2><p><a href="https://blog.csdn.net/qq_43447339/article/details/135758451">zsh配置</a></p><h2 id="三、软件、环境等配置"><a href="#三、软件、环境等配置" class="headerlink" title="三、软件、环境等配置"></a>三、软件、环境等配置</h2><h3 id="1-ROS2配置"><a href="#1-ROS2配置" class="headerlink" title="1. ROS2配置"></a>1. ROS2配置</h3><p><a href="https://fishros.org.cn/forum/topic/20/%E5%B0%8F%E9%B1%BC%E7%9A%84%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85%E7%B3%BB%E5%88%97?lang=zh-CN">ROS2一键安装教程</a></p><h3 id="2-深度学习环境配置"><a href="#2-深度学习环境配置" class="headerlink" title="2. 深度学习环境配置"></a>2. 深度学习环境配置</h3><h4 id="1-CUDA安装"><a href="#1-CUDA安装" class="headerlink" title="(1) CUDA安装"></a>(1) CUDA安装</h4><ul><li>软件更新—&gt;附加驱动</li><li>ubuntu 20.04 cuda12.0安装命令<br><a href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA下载地址</a><br><a href="https://blog.csdn.net/weixin_37926734/article/details/123033286">参考博客</a></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin<br>sudo <span class="hljs-built_in">mv</span> cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600<br>wget https://developer.download.nvidia.com/compute/cuda/12.0.0/local_installers/cuda-repo-ubuntu2004-12-0-local_12.0.0-525.60.13-1_amd64.deb<br>sudo dpkg -i cuda-repo-ubuntu2004-12-0-local_12.0.0-525.60.13-1_amd64.deb<br>sudo <span class="hljs-built_in">cp</span> /var/cuda-repo-ubuntu2004-12-0-<span class="hljs-built_in">local</span>/cuda-*-keyring.gpg /usr/share/keyrings/<br>sudo apt-get update<br>sudo apt-get -y install cuda<br></code></pre></td></tr></table></figure><h4 id="2-Conda环境配置"><a href="#2-Conda环境配置" class="headerlink" title="(2) Conda环境配置"></a>(2) Conda环境配置</h4><ul><li>关闭自动激活的base环境</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda config --<span class="hljs-built_in">set</span> auto_activate_base <span class="hljs-literal">false</span><br></code></pre></td></tr></table></figure><h4 id="3-Git配置"><a href="#3-Git配置" class="headerlink" title="(3) Git配置"></a>(3) Git配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">git安装与卸载<br>apt-get install git apt-get remove git<br><br>git配置<br>配置用户名<br>git config --global user.name “zl”<br>配置邮箱<br>git config --global user.email “zlsy@mail.ustc.edu.cn”<br>查看配置信息<br>git config --global --list<br>生成公钥<br>ssh-keygen -t rsa -C <span class="hljs-string">&quot;zlsy@mail.ustc.edu.cn&quot;</span><br></code></pre></td></tr></table></figure><h4 id="4-pip镜像源配置"><a href="#4-pip镜像源配置" class="headerlink" title="(4) pip镜像源配置"></a>(4) pip镜像源配置</h4><ul><li>临时使用pip镜像源</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo pip install numpy -i https://pypi.tuna.tsinghua.edu.cn/simple/ <br></code></pre></td></tr></table></figure><ul><li>永久配置</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> ~/.pip  <span class="hljs-comment">#创建一个名为.pip的文件夹</span><br><span class="hljs-built_in">cd</span> ~/.pip    <span class="hljs-comment">#进入创建的文件夹</span><br><span class="hljs-built_in">touch</span> pip.conf  <span class="hljs-comment">#创建pip.conf</span><br>sudo gedit ~/.pip/pip.conf   <span class="hljs-comment">#编辑文件</span><br> <br>复制下面的内容到文件中（配置的豆瓣源，也可以配置别的）<br>[global]<br>index-url = https://pypi.mirrors.ustc.edu.cn/simple/<br>[install]<br>trusted-host=pypi.mirrors.ustc.edu.cn<br><br>镜像源推荐<br>（1）阿里云 https://mirrors.aliyun.com/pypi/simple/<br>（2）豆瓣https://pypi.douban.com/simple/<br>（3）清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/<br>（4）中国科学技术大学 https://pypi.mirrors.ustc.edu.cn/simple/<br>（5）华中科技大学https://pypi.hustunique.com/<br></code></pre></td></tr></table></figure><h3 id="3-软件安装"><a href="#3-软件安装" class="headerlink" title="3.软件安装"></a>3.软件安装</h3><h4 id="CoppeliaSim"><a href="#CoppeliaSim" class="headerlink" title="CoppeliaSim"></a>CoppeliaSim</h4><ul><li><a href="https://blog.csdn.net/qq_43447339/article/details/135889186">创建快捷方式</a></li><li>终端打开（推荐）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">alias</span> vrep=<span class="hljs-string">&#x27;/opt/CoppeliaSim/CoppeliaSim_Edu_V4_1_0_Ubuntu20_04/coppeliaSim.sh&#x27;</span><br></code></pre></td></tr></table></figure><h4 id="Mujoco"><a href="#Mujoco" class="headerlink" title="Mujoco"></a>Mujoco</h4><ul><li><a href="https://blog.csdn.net/weixin_51844581/article/details/128454472">Ubuntu20.04安装Mujoco</a></li><li>相关问题<br>—————————-完善中——————————-</li></ul>]]></content>
    
    
    <categories>
      
      <category>OS</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Ubuntu</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mujoco 学习笔记</title>
    <link href="/2024/05/25/mujoco_learn/"/>
    <url>/2024/05/25/mujoco_learn/</url>
    
    <content type="html"><![CDATA[<h2 id="一、Mujoco安装"><a href="#一、Mujoco安装" class="headerlink" title="一、Mujoco安装"></a>一、Mujoco安装</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pip install mujoco<br></code></pre></td></tr></table></figure><h2 id="二、交互式查看器"><a href="#二、交互式查看器" class="headerlink" title="二、交互式查看器"></a>二、交互式查看器</h2><h3 id="命令行打开"><a href="#命令行打开" class="headerlink" title="命令行打开"></a>命令行打开</h3><ul><li>启动一个空的可视化会话，其中可以通过拖放加载模型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python -m mujoco.viewer<br></code></pre></td></tr></table></figure><ul><li>指定加载模型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python -m mujoco.viewer --mjcf=/path/to/some/mjcf.xml<br></code></pre></td></tr></table></figure><h3 id="Python程序中打开"><a href="#Python程序中打开" class="headerlink" title="Python程序中打开"></a>Python程序中打开</h3><ul><li><code>viewer.launch()</code></li></ul><p>启动一个空的可视化会话，其中可以通过拖放加载模型。</p><ul><li><code>viewer.launch(model)</code></li></ul><p>可视化工具启动给定的可视化会话 在内部创建自己的实例</p><ul><li><code>viewer.launch(model, data)</code></li></ul><p>与上述相同，只是可视化工具直接在给定实例上运行，退出时对象将被修改。</p><h3 id="被动查看器"><a href="#被动查看器" class="headerlink" title="被动查看器"></a>被动查看器</h3><ul><li><code>viewer.launch_passive(model, data)</code></li></ul><p>创建一个被动查看器实例。在此模式下，函数不会阻塞，允许代码继续执行。用户的脚本负责计时和推进物理状态，除非用户显式同步传入事件，否则鼠标拖动将不起作用</p><h2 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> mujoco<br></code></pre></td></tr></table></figure><h2 id="参考-1"><a href="#参考-1" class="headerlink" title="参考[1]"></a>参考<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Mujoco Python](https://mujoco.readthedocs.io/en/stable/python.html#)">[1]</span></a></sup></h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://mujoco.readthedocs.io/en/stable/python.html#">Mujoco Python</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Mujoco</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Mujoco</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Windows/Ubuntu22.04 解决CoppeliaSim界面字体过小问题</title>
    <link href="/2024/05/25/coppeliasim/"/>
    <url>/2024/05/25/coppeliasim/</url>
    
    <content type="html"><![CDATA[<h2 id="Ubuntu22-04-解决VREP-CoppeliaSim-界面字体过小问题"><a href="#Ubuntu22-04-解决VREP-CoppeliaSim-界面字体过小问题" class="headerlink" title="Ubuntu22.04 解决VREP&#x2F;CoppeliaSim 界面字体过小问题"></a>Ubuntu22.04 解决VREP&#x2F;CoppeliaSim 界面字体过小问题</h2><h3 id="Windows10"><a href="#Windows10" class="headerlink" title="Windows10"></a>Windows10</h3><p>1.右键单击CoppeliaSim.exe（可能的位置：C：\ Program Files \ CoppeliaRobotics \ CoppeliaSimEdu），然后单击“属性”。</p><p>2.在“兼容性”选项卡下</p><p>3.单击“更改高DPI设置”</p><p>4.选中“替代高DPI缩放行为”框</p><p>5.缩放比例–选择“系统”</p><h3 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu"></a>Ubuntu</h3><p>1.找到CoppeliaSim安装位置（例如：&#x2F;opt&#x2F;CoppeliaSimo_Edu_V4_2_0_Ubuntu20_04）<br>2.打开system 文件夹下usrset.txt文件，将变量“ highhandedness”的值更改为2<br>3.重启CoppeliaSim<br><img src="/../img/blogs/coppeliasim.png" alt="image"></p>]]></content>
    
    
    <categories>
      
      <category>CoppeliaSim</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Coppeliasim</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/05/25/hello-world/"/>
    <url>/2024/05/25/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
