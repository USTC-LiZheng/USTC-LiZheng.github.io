<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>预训练模型</title>
    <link href="/2024/08/19/nlp/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/08/19/nlp/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="预训练模型"><a href="#预训练模型" class="headerlink" title="预训练模型"></a>预训练模型</h1><h2 id="理解形式"><a href="#理解形式" class="headerlink" title="理解形式"></a>理解形式</h2><div style="display: flex; justify-content: space-between;">    <img src="../../img/blogs/nlp/NLP_intro/nlp-intro5.png" width="33%" />    <img src="../../img/blogs/nlp/NLP_intro/nlp-intro6.png" width="33%" />    <img src="../../img/blogs/nlp/NLP_intro/nlp-intro6.png" width="33%" /></div><!-- <img src="../../img/blogs/nlp/NLP_intro/nlp-intro6.png"      width="500" height="300"      style="display: block; margin: 0 auto;" /> --><ul><li>我们需要找到一种方式将我们熟知的中文，英文和各种外文转化成数字形式。自然语言处理中有一个典型的应用，就是翻译。</li><li>将一种语言作为输入，一种作为输出，使用NLP作为中间的桥梁，首先将中文通过一种压缩机制转码成机器能理解的数字，然后用中间这种数字化的语言表达形式，再通过一次英文的解压，解压出来英文作为输出语言。</li></ul><h2 id="对话"><a href="#对话" class="headerlink" title="对话"></a>对话</h2><p><img src="../../img/blogs/nlp/NLP_intro/nlp-intro8.png"     width="500" height="300"     style="display: block; margin: 0 auto;" /></p><ul><li>当和计算机对话，计算机在收到你的语言信息后，会翻译成它能理解的数字内容，然后使用这些数字语言，通过一些处理分析，做出行为决策，最终返回人类的语言。一来一回，形成对话，解决具体问题。</li></ul>]]></content>
    
    
    <categories>
      
      <category>nlp</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>注意力机制</title>
    <link href="/2024/08/19/nlp/5-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <url>/2024/08/19/nlp/5-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h1><h2 id="理解形式"><a href="#理解形式" class="headerlink" title="理解形式"></a>理解形式</h2><div style="display: flex; justify-content: space-between;">    <img src="../../img/blogs/nlp/NLP_intro/nlp-intro5.png" width="33%" />    <img src="../../img/blogs/nlp/NLP_intro/nlp-intro6.png" width="33%" />    <img src="../../img/blogs/nlp/NLP_intro/nlp-intro6.png" width="33%" /></div><!-- <img src="../../img/blogs/nlp/NLP_intro/nlp-intro6.png"      width="500" height="300"      style="display: block; margin: 0 auto;" /> --><ul><li>我们需要找到一种方式将我们熟知的中文，英文和各种外文转化成数字形式。自然语言处理中有一个典型的应用，就是翻译。</li><li>将一种语言作为输入，一种作为输出，使用NLP作为中间的桥梁，首先将中文通过一种压缩机制转码成机器能理解的数字，然后用中间这种数字化的语言表达形式，再通过一次英文的解压，解压出来英文作为输出语言。</li></ul><h2 id="对话"><a href="#对话" class="headerlink" title="对话"></a>对话</h2><p><img src="../../img/blogs/nlp/NLP_intro/nlp-intro8.png"     width="500" height="300"     style="display: block; margin: 0 auto;" /></p><ul><li>当和计算机对话，计算机在收到你的语言信息后，会翻译成它能理解的数字内容，然后使用这些数字语言，通过一些处理分析，做出行为决策，最终返回人类的语言。一来一回，形成对话，解决具体问题。</li></ul>]]></content>
    
    
    <categories>
      
      <category>nlp</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>更多应用</title>
    <link href="/2024/08/19/nlp/7-%E6%9B%B4%E5%A4%9A%E5%BA%94%E7%94%A8/"/>
    <url>/2024/08/19/nlp/7-%E6%9B%B4%E5%A4%9A%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="更多应用"><a href="#更多应用" class="headerlink" title="更多应用"></a>更多应用</h1>]]></content>
    
    
    <categories>
      
      <category>nlp</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>理解词语</title>
    <link href="/2024/08/19/nlp/3-%E7%90%86%E8%A7%A3%E8%AF%8D%E8%AF%AD/"/>
    <url>/2024/08/19/nlp/3-%E7%90%86%E8%A7%A3%E8%AF%8D%E8%AF%AD/</url>
    
    <content type="html"><![CDATA[<h1 id="理解词语"><a href="#理解词语" class="headerlink" title="理解词语"></a>理解词语</h1><h2 id="机器是这样理解语言-词向量"><a href="#机器是这样理解语言-词向量" class="headerlink" title="机器是这样理解语言 - 词向量"></a>机器是这样理解语言 - 词向量</h2><h3 id="1-计算机理解语言"><a href="#1-计算机理解语言" class="headerlink" title="1. 计算机理解语言"></a>1. 计算机理解语言</h3><p><img     src="../../img/blogs/nlp/理解词语/nlp-w2v3.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/><br><img     src="../../img/blogs/nlp/理解词语/nlp-w2v4.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/></p><ul><li>计算机之所以能看懂字里行间的感情，理解文字，处理文字，并不是因为它理解的我们普罗万象的人类语言，而是它将语言或者词汇归类到了一个正确的位置上。计算机对词语的理解，其实是计算机对空间及位置的理解。</li><li>不管是图片，文章，句子，词语，声音，只要是能被数值化，被投射到某个空间中，计算机都能把它们按相似度聚集起来。<br><img   src="../../img/blogs/nlp/理解词语/nlp-w2v6.png"  width="500" height="300"  style="display: block; margin: 0 auto;"/></li></ul><h3 id="2-词向量"><a href="#2-词向量" class="headerlink" title="2. 词向量"></a>2. 词向量</h3><p><img     src="../../img/blogs/nlp/理解词语/nlp-w2v7.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/><br>在词向量训练的过程中，相似词总会被聚集到一块地方且方向大概都相同，比如这里的猫狗龙。而差异较大的词会被拉远，且方向有可能不同，所以和猫狗这些动物相比，没有生命的飞机，碗等都离得很远。</p><p>所以如果只想测量两个词的相似度，角度信息也足够了。但点与点的距离还透露了更多的信息，只要两个词总在一起出现，他们之间的关联性应该越强，距离应该也越近。我们想一想，如果一个词不仅出现的频率高，而且任何句子中都能出现，比如“在”，“你”，“吗”这一类的词。</p><p><img     src="../../img/blogs/nlp/理解词语/nlp-w2v8.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/><br>为了得到这些词的位置，机器需要不断计算他们之间的相关性。这个过程称之为机器学习或者模型训练。这些词每次训练的时候都想被拉扯到独立的空间，但是被太多不同方向的词拉来拉去，比如”在”这个字，训练“在这”的时候“在”字被拉扯到靠近“这”字的方向。训练“在家”的时候，“在”字将会更靠近“家”字，后面的训练也一样，所以“在”字因为频率太高，和很多字都能混搭，它就算是之中机器认为的“中性词”，越有区分力的词可能越远离中心地带，因为他们和其他词都不像，而越通用，在每种场景都有的词，就可能越靠近原点。这时，点与点的距离就能告诉我们词的频率性特征。</p><h3 id="3-训练词向量"><a href="#3-训练词向量" class="headerlink" title="3. 训练词向量"></a>3. 训练词向量</h3><p><img     src="../../img/blogs/nlp/理解词语/nlp-w2v9.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/><br>训练词向量可以直接在原始语料上做非监督学习，只要有各种各样的文章数据就行。训练时，取一小段文本，取出这些词的向量表示，比如取出除了“一”字以外的词向量，然后整合到一起，表示这些文字的整体向量，用这个整体向量预测最中间那个“一”。接下来在开始下一段文字的训练。</p><p>将这个窗口挪动一格，用前后文预测“段”字，接着将窗口依次这样扫过所有文字，用所有的前后文预测中间词，这样计算机就能将前后文的关系搞清楚，挨得近的词他们的关系越亲密总出现在类似的上下文中间的词关系越亲密。向量在一定程度上也越相近。除了用前后文预测中间词，我们还能换一个思路，用中间词预测前后文也行。</p><h3 id="4-词向量的应用"><a href="#4-词向量的应用" class="headerlink" title="4. 词向量的应用"></a>4. 词向量的应用</h3><p><img     src="../../img/blogs/nlp/理解词语/nlp-w2v10.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/><br>有种用法很简单，就是直接把词向量当成词语特征输入进另一个模型里。这样就能用更丰富的词向量信息来表示一个词语ID。在这种情况中，我们说词向量是一种预训练特征。用word2vec 的方法预先训练好了词语的特征表达，然后在其他场景中拿着预训练结果直接使用。</p><div style="display: flex; justify-content: space-between;">    <img src="../../img/blogs/nlp/理解词语/nlp-w2v11.png" width="50%" />    <img src="../../img/blogs/nlp/理解词语/nlp-w2v12.png" width="50%" /></div>还有种更有趣的玩法，用词向量进行加减运算，男人减掉女人的词向量，差不多就约等于公猫减掉母猫的词向量。<h2 id="Continuous-Bag-of-Words-CBOW"><a href="#Continuous-Bag-of-Words-CBOW" class="headerlink" title="Continuous Bag of Words (CBOW)"></a>Continuous Bag of Words (CBOW)</h2><h3 id="1-CBOW模型"><a href="#1-CBOW模型" class="headerlink" title="1. CBOW模型"></a>1. CBOW模型</h3><p>用一句话概述：挑一个要预测的词，来学习这个词前后文中词语和预测词的关系。</p><p><code>举个例子: 我爱莫烦Python，莫烦Python通俗易懂。</code></p><p>模型在做的事情如图中所示，将这句话拆成输入和输出，用前后文的词向量来预测句中的某个词。<br><img     src="../../img/blogs/nlp/理解词语/cbow_illustration.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/><br>通过在大数据量的短语或文章中学习这样的词语关系，这个模型就能理解要预测的词和前后文的关系。而图中彩色的词向量就是这种训练过程的一个副产品。</p><h3 id="2-词向量的应用"><a href="#2-词向量的应用" class="headerlink" title="2. 词向量的应用"></a>2. 词向量的应用</h3><p>词向量的几种典型应用：</p><ul><li>把这些对词语理解的向量通过特定方法组合起来，就可以有对某句话的理解了；</li><li>可以在向量空间中找寻同义词，因为同义词表达的意思相近，往往在空间中距离也非常近；</li><li>词语的距离换算。<br>词语距离计算这个比较有意思，比如可以拿词语做加减法。公猫 - 母猫 就约等于 男人 - 女人。</li></ul><h2 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip-Gram"></a>Skip-Gram</h2><h3 id="1-Skip-Gram模型"><a href="#1-Skip-Gram模型" class="headerlink" title="1. Skip-Gram模型"></a>1. Skip-Gram模型</h3><p>Skip-Gram 是把CBOW过程反过来,使用文中的某个词，然后预测这个词周边的词。</p><div style="display: flex; justify-content: space-between;">    <img src="../../img/blogs/nlp/理解词语/cbow_illustration.png" width="50%" />    <img src="../../img/blogs/nlp/理解词语/skip_gram_illustration.png" width="50%" /></div><ul><li>Skip-Gram 相比 CBOW 最大的不同，就是剔除掉了中间的那个 SUM 求和的过程，我在这里提到过, 我觉得将词向量求和的这个过程不太符合直观的逻辑，因为我也不知道这加出来的到底代表着是一个句向量还是一个另词向量，求和是一种粗暴的类型转换。 而Skip-Gram没有这个过程，最终我们加工的始终都是输入端单个词向量，这样的设计我比较能够接受。</li></ul>]]></content>
    
    
    <categories>
      
      <category>nlp</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>理解句子</title>
    <link href="/2024/08/19/nlp/4-%E7%90%86%E8%A7%A3%E5%8F%A5%E5%AD%90/"/>
    <url>/2024/08/19/nlp/4-%E7%90%86%E8%A7%A3%E5%8F%A5%E5%AD%90/</url>
    
    <content type="html"><![CDATA[<h1 id="理解句子"><a href="#理解句子" class="headerlink" title="理解句子"></a>理解句子</h1><h2 id="理解形式"><a href="#理解形式" class="headerlink" title="理解形式"></a>理解形式</h2><div style="display: flex; justify-content: space-between;">    <img src="../../img/blogs/nlp/NLP_intro/nlp-intro5.png" width="33%" />    <img src="../../img/blogs/nlp/NLP_intro/nlp-intro6.png" width="33%" />    <img src="../../img/blogs/nlp/NLP_intro/nlp-intro6.png" width="33%" /></div><!-- <img src="../../img/blogs/nlp/NLP_intro/nlp-intro6.png"      width="500" height="300"      style="display: block; margin: 0 auto;" /> --><ul><li>我们需要找到一种方式将我们熟知的中文，英文和各种外文转化成数字形式。自然语言处理中有一个典型的应用，就是翻译。</li><li>将一种语言作为输入，一种作为输出，使用NLP作为中间的桥梁，首先将中文通过一种压缩机制转码成机器能理解的数字，然后用中间这种数字化的语言表达形式，再通过一次英文的解压，解压出来英文作为输出语言。</li></ul><h2 id="对话"><a href="#对话" class="headerlink" title="对话"></a>对话</h2><p><img src="../../img/blogs/nlp/NLP_intro/nlp-intro8.png"     width="500" height="300"     style="display: block; margin: 0 auto;" /></p><ul><li>当和计算机对话，计算机在收到你的语言信息后，会翻译成它能理解的数字内容，然后使用这些数字语言，通过一些处理分析，做出行为决策，最终返回人类的语言。一来一回，形成对话，解决具体问题。</li></ul>]]></content>
    
    
    <categories>
      
      <category>nlp</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP简介</title>
    <link href="/2024/08/19/nlp/1-nlp%E7%AE%80%E4%BB%8B/"/>
    <url>/2024/08/19/nlp/1-nlp%E7%AE%80%E4%BB%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="NLP简介"><a href="#NLP简介" class="headerlink" title="NLP简介"></a>NLP简介</h1><h2 id="理解形式"><a href="#理解形式" class="headerlink" title="理解形式"></a>理解形式</h2><div style="display: flex; justify-content: space-between;">    <img src="../../img/blogs/nlp/NLP_intro/nlp-intro5.png" width="33%" />    <img src="../../img/blogs/nlp/NLP_intro/nlp-intro6.png" width="33%" />    <img src="../../img/blogs/nlp/NLP_intro/nlp-intro6.png" width="33%" /></div><!-- <img src="../../img/blogs/nlp/NLP_intro/nlp-intro6.png"      width="500" height="300"      style="display: block; margin: 0 auto;" /> --><ul><li>我们需要找到一种方式将我们熟知的中文，英文和各种外文转化成数字形式。自然语言处理中有一个典型的应用，就是翻译。</li><li>将一种语言作为输入，一种作为输出，使用NLP作为中间的桥梁，首先将中文通过一种压缩机制转码成机器能理解的数字，然后用中间这种数字化的语言表达形式，再通过一次英文的解压，解压出来英文作为输出语言。</li></ul><h2 id="对话"><a href="#对话" class="headerlink" title="对话"></a>对话</h2><p><img src="../../img/blogs/nlp/NLP_intro/nlp-intro8.png"     width="500" height="300"     style="display: block; margin: 0 auto;" /></p><ul><li>当和计算机对话，计算机在收到你的语言信息后，会翻译成它能理解的数字内容，然后使用这些数字语言，通过一些处理分析，做出行为决策，最终返回人类的语言。一来一回，形成对话，解决具体问题。</li></ul>]]></content>
    
    
    <categories>
      
      <category>nlp</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>搜索</title>
    <link href="/2024/08/19/nlp/2-%E6%90%9C%E7%B4%A2/"/>
    <url>/2024/08/19/nlp/2-%E6%90%9C%E7%B4%A2/</url>
    
    <content type="html"><![CDATA[<h1 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h1><h2 id="搜索引擎工作原理"><a href="#搜索引擎工作原理" class="headerlink" title="搜索引擎工作原理"></a>搜索引擎工作原理</h2><h3 id="1-搜索引擎"><a href="#1-搜索引擎" class="headerlink" title="1. 搜索引擎"></a>1. 搜索引擎</h3><p><img     src="../../img/blogs/nlp/搜索/nlp-search2.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/></p><ul><li>在早期互联网时代，世界上诞生了一个影响全世界的公司，Google，它所带来的搜索也突破老一代的技术，登上了历史舞台，并影响着后续2-30年的人类社会发展。华人比较熟悉的百度则是在稍后一些，为中文搜索带来了前所未有的体验。</li></ul><h3 id="2-垂类搜索"><a href="#2-垂类搜索" class="headerlink" title="2. 垂类搜索"></a>2. 垂类搜索</h3><p><img     src="../../img/blogs/nlp/搜索/nlp-search3.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/></p><ul><li>搜个社交，点个外卖，查个账款，旅个游，这些生活中的服务体系，早已经深度嵌入了我们一天中每一个时间点，对应上了会发生在我们生活的每一个行为。当别人在谈论搜索引擎的时候，可能已经不再是谈论google百度这样的搜索入口了，那千千万万个长在垂类应用里的搜索，才是组成我们今天生活获取信息的主要入口。</li></ul><h3 id="3-构建索引"><a href="#3-构建索引" class="headerlink" title="3. 构建索引"></a>3. 构建索引</h3><p><img     src="../../img/blogs/nlp/搜索/nlp-search5.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/></p><ul><li><p>在一篇文章可以被搜索之前，搜索引擎将对他们进行细致的观察，因为它可不想把全部的信息，不分重点的一股脑存储下来。取而代之，它会挑选重点部分，分别对待，比如重点关注标题、时间、正文。将这些信息给予不同的权重后，接着就是下一步，将它存储起来。</p></li><li><p>搜索引擎通常在搜索的时候，不会临时从全网找材料，而是将刚刚收集到的信息提前构建成索引，存储在便于快速检索的数据库中。只在自己的数据库中搜索，使我们的及时搜索更有效率。</p></li></ul><h3 id="4-数值匹配搜索"><a href="#4-数值匹配搜索" class="headerlink" title="4. 数值匹配搜索"></a>4. 数值匹配搜索</h3><p><img     src="../../img/blogs/nlp/搜索/nlp-search6.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/></p><ul><li><p>现在的深度学习给我们提供了另一条思路，也就是用模型从非文字的信息中提取计算机能够识别的可计算信息。 在用户用文字搜索时，将搜索的文字内容转换成深度学习能识别的数字内容，然后再和之前存储的图片、视频数字信息进行匹配，对比两种数字之间的关联性，然后找到最相近的内容。这种搜索，我们有一个专业名词叫作多模态搜索。</p></li><li><p>多模态搜索并不仅限于文字搜图片视频，它还能颠倒过来，用图片搜图片，图片搜视频等，因为在深度学习看来，只要它们能被转换成统一的数字形态，我就能对比相似性。</p></li></ul><h3 id="5-搜索过滤"><a href="#5-搜索过滤" class="headerlink" title="5. 搜索过滤"></a>5. 搜索过滤</h3><p><img     src="../../img/blogs/nlp/搜索/nlp-search7.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/></p><ul><li><p>深度学习模型有一个速度的硬伤，相比传统方法，每一次预测需要消耗更多的时间。而用户是无法忍受搜索中的延迟。为了实现在海量网页和文件中的快速搜索遍历，我们不得不使用到更加传统的方法, 而把深度学习方法放到后续更加适合的步骤中。</p></li><li><p>目前比较常用的方式，类似于这里的层层筛选过滤的方式，将筛选结果用不同的方法，从海量的网页中，一层层过滤到最符合你搜索条件的结果。而在需要做大量文档过滤处理的阶段，我们就使用时间消耗相对较少的技术，最后可以把深度学习方案，放在文档量和计算量都少的地方。接下来我们就介绍一下，什么样的技术能在数据量庞大的地方，又快又准地帮你找到搜索内容。</p></li></ul><h3 id="6-正排-倒排索引"><a href="#6-正排-倒排索引" class="headerlink" title="6. 正排&#x2F;倒排索引"></a>6. 正排&#x2F;倒排索引</h3><p><img     src="../../img/blogs/nlp/搜索/nlp-search8.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/></p><ul><li>我们总说NLP会让计算机懂得文字的内涵，但是有时候，有更加投机取巧的方法可以让计算机在不理解文字内涵的时候，还能给我们快速带来准确的结果。特别是在搜索中不得不提到的倒排索引技术。倒排索引是一种批量召回技术，它能快速在海量数据中初步召回基本符合要求的文章。</li></ul><p><img     src="../../img/blogs/nlp/搜索/nlp-search9.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/></p><p>假设你开了家咨询公司，手上有100篇材料。这时有人来找你咨询NLP的问题，你会怎么在这100篇材料中找到合适的内容呢？</p><ul><li><p>方法1：我们一篇一篇地阅读，找到所有包含NLP内容的材料，然后返回给提问者。这种方法需要我们在每次搜索的时候，都对所有材料进行一次阅读，然后在材料中找到关键词，并筛选出材料，效率其实非常差。</p></li><li><p>方法2：我们在第一次拿到所有材料时，把它们通读一遍，然后构建关键词和文章的对应关系。当用户在搜索特定词的时候，比如“红”，就会直接返回“红”这个【关键词索引】下的文章列表。先构造索引的好处就是能够将这种索引，放在后续的搜索中复用，搜索也就变成了一种词语匹配加返回索引材料的过程。</p></li></ul><p>这里的 方式1是我们所谓的正排索引，方式2是更加快速的倒排索引。但当处理的是海量数据的时候，通过倒排索引找到的文章可能依然是海量。如果能有种方法对这些文章进行排序操作，再选取排名靠前的文章列表也能帮我们节省大量的时间。处理匹配排序，最有名的算法之一叫做TF-IDF。</p><h3 id="7-TF-IDF"><a href="#7-TF-IDF" class="headerlink" title="7. TF-IDF"></a>7. TF-IDF</h3><p><img     src="../../img/blogs/nlp/搜索/nlp-search10.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/></p><p>先来看看TF-IDF所处的位置是哪里吧，有了批量性地召回相对合适的内容后，比如我已经从1亿个网页中召回了100万个，但100万对于我来说，已经够让我看上好几年了。怎么能再继续提升一下精确度，找到我更在乎的内容呢？</p><p>那么对筛选出来的内容做一个【问题与内容】的相似度排序，只返回那些头部内容就好啦。这个工作显然还是有一定的计算量的，所以如果前面不做召回，在1亿个网页中直接用打分排序的逻辑，往往还是挺久的。所以最好是将召回作为初步筛选，然后再相似度打分找到我在乎的内容，从而减轻计算负担。</p><p><img     src="../../img/blogs/nlp/搜索/nlp-search11.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/><br>如果说TF是以文章为中心的局部词信息，那么IDF则是全局的词信息。在一篇文章中，越重要的内容，强调的次数也越多，所以<strong>频率</strong>(TF)会大，我们可以用词频高的词代表这篇文章。所以TF可以用一张词和文章标号的表来展示。不过问题来了，像语气词或“你我他”这种词，同样也会出现很多次，光用TF，我们没办法除去这些词的影响。而TF-IDF中的IDF，就可以在这个时候帮上忙，它是所有词在这个系统中的区分力的大小，如果每篇文章里都有“我”这个字，那么它的在任意一篇文章当中的区分力都不强，而如果你关键词搜索的是“莫烦”，那么全网都没有几个叫“莫烦”的，“莫烦”IDF就会很大，意味着“莫烦”的区分力也够强。</p><p><img     src="../../img/blogs/nlp/搜索/nlp-search12.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/><br>TF-IDF 两者结合其实就是两者相乘的意思，这样的结果意味着所有的文章，都能用一串集合所有词的分数来表示。通过分数的高低，我们也能大概看出这篇文章的关键内容是什么。</p><p><img     src="../../img/blogs/nlp/搜索/nlp-search13.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/><br>假设我们搜索关键词“莫烦Python”,机器会利用词表的模式计算“莫烦Python”这个问题的TF-IDF值。然后会计算问句和每篇文章的cosine距离，这个例子中的计算过程，简单来说，就是将文章按照词的维度放到一个四维空间中，然后把问句同样也放到这个空间里，最后看空间中这个问题离哪一个文章的距离最近，越近则相似度越高。通过这样的方式呢，我们就能找到搜索问题的最佳匹配文章了。</p><p><img     src="../../img/blogs/nlp/搜索/nlp-search14.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/><br>举个例子，第一串数字就是文章1的向量表达，第二串是文章2的向量表达，第三串是问题的向量表达。他们都是空间中的点。</p><h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><h3 id="1-TF-IDF-的数学表达形式"><a href="#1-TF-IDF-的数学表达形式" class="headerlink" title="1. TF-IDF 的数学表达形式"></a>1. TF-IDF 的数学表达形式</h3><p>其实它就是一个庞大的矩阵，用词语的数字向量来代表一篇文档，当比较文档时，就是在比较这些向量的相似性。比如下面有三篇文档，每篇文档中都不断重复着莫烦，Python，最棒这三个词。 通过对每个词计算TF-IDF后，我们可以用这这些TF-IDF值代表这三篇文档，也就是说，每篇文档就是一个三维向量。</p><table><thead><tr><th align="center">词语</th><th align="center">文档1</th><th align="center">文档2</th><th align="center">文档3</th></tr></thead><tbody><tr><td align="center">莫烦</td><td align="center">0.5</td><td align="center">0.3</td><td align="center">0.2</td></tr><tr><td align="center">Python</td><td align="center">0.4</td><td align="center">0.4</td><td align="center">0.4</td></tr><tr><td align="center">最棒</td><td align="center">0.1</td><td align="center">0.3</td><td align="center">0.4</td></tr></tbody></table><p>如果把向量展示在图上，就会有类似下面图案的样子，加上一个计算向量相似度的方式，比如cosine距离，我们就能判段哪些文档在这个三维空间上比较相似了。 图中两个蓝色向量离得越近，就代表他们越像。</p><p><img     src="../../img/blogs/nlp/搜索/tfidf_doc_vec.png"    width="500" height="300"    style="display: block; margin: 0 auto;"/></p><h2 id="搜索的扩展"><a href="#搜索的扩展" class="headerlink" title="搜索的扩展"></a>搜索的扩展</h2><p>用TF-IDF来表示文档，然后可以用来做搜索和提取关键词。 可是在代码中存在一个机制，会引发内存占用大的问题。</p><p>TF-IDF是一张二维表，分别代表文章索引和单词索引。文章量是可以无限增大的，单词量的增长也是很恐怖的。那么随着这两个维度的增长， 我们的内存总有一天会扛不住。好在我们可以利用一个节约内存的技术，叫做Sparse Matrix，稀疏矩阵，它只会存储有内容的值，而忽略无内容的值。 在这张巨大的二维表中，肯定每篇文章不一定会提及到所有词汇，这些不提及的词汇，我们当然可以不用存储。</p><p>用 Skearn 模块的 Sparse Matrix 功能，能更快速，有效地计算和存储海量的数据。</p>]]></content>
    
    
    <categories>
      
      <category>nlp</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Policy Gradients</title>
    <link href="/2024/08/05/rl/4-Policy%20Gradients/"/>
    <url>/2024/08/05/rl/4-Policy%20Gradients/</url>
    
    <content type="html"><![CDATA[<h1 id="Policy-Gradients"><a href="#Policy-Gradients" class="headerlink" title="Policy Gradients"></a>Policy Gradients</h1><ul><li>对比起<code>Value-based</code>的方法(<code>Q learning, Deep Q Network</code>), Policy Gradients 直接输出动作，最大好处就是, 它能在一个连续分布上选取 action。</li></ul><h2 id="Policy-Gradients-决策"><a href="#Policy-Gradients-决策" class="headerlink" title="Policy Gradients 决策"></a>Policy Gradients 决策</h2><ul><li>行为不再是<code>Q-value</code>来选定的, 而是用<em><strong>概率</strong></em>来选定.</li></ul><h2 id="Policy-Gradients-更新"><a href="#Policy-Gradients-更新" class="headerlink" title="Policy Gradients 更新"></a>Policy Gradients 更新</h2><p><img src="/../../img/blogs/rl/PG/PG02.png" alt="1720667162152"></p><ul><li>观测的信息通过神经网络分析, 选出了左边的行为, 直接进行反向传递, 使之下次被选的可能性增加, 但是奖惩信息却告诉这次的行为是不好的, 那动作可能性增加的幅度随之被减低. 这样就能靠奖励来左右神经网络反向传递。(回合更新)</li></ul><h2 id="Policy-Gradients整体算法"><a href="#Policy-Gradients整体算法" class="headerlink" title="Policy Gradients整体算法"></a>Policy Gradients整体算法</h2><p><img src="/../../img/blogs/rl/PG/PG03.png" alt="1720667162152"><br><img src="/../../img/blogs/rl/PG/PG04.png" alt="1720667162152"></p><ul><li><strong>吃惊度</strong>：<code>(log(Policy(s,a))*V)</code> 表示在 状态 s 对所选动作 a 的<em><strong>吃惊度</strong></em>, 如果<code>Policy(s,a)</code>概率越小, 反向的<code>log(Policy(s,a))</code>(即 -log(P)) 反而越大。如果在<code>Policy(s,a)</code>很小的情况下, 拿到了一个大的<code>R</code>, 也就是大的<code>V</code>, 那<code>-(log(Policy(s, a))*V)</code>就更大, 表示更吃惊, (我选了一个不常选的动作, 却发现原来它能得到了一个好的<code>reward</code>, 那我就得对我这次的参数进行一个大幅修改)。</li></ul>]]></content>
    
    
    <categories>
      
      <category>RL算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PPO</title>
    <link href="/2024/08/05/rl/7-PPO/"/>
    <url>/2024/08/05/rl/7-PPO/</url>
    
    <content type="html"><![CDATA[<h1 id="Proximal-Policy-Optimization（PPO）"><a href="#Proximal-Policy-Optimization（PPO）" class="headerlink" title="Proximal Policy Optimization（PPO）"></a>Proximal Policy Optimization（PPO）</h1><ul><li>一句话概括<code>PPO</code>: OpenAI 提出的一种解决<code>Policy Gradient</code>不好确定<code>Learning rate</code>(或者<code>Step size</code>) 的问题。 因为如果<code>step size</code>过大, 学出来的<code>Policy</code>会一直乱动, 不会收敛, 但如果<code>Step Size</code>太小, 对于完成训练, 我们会等到绝望。 <code>PPO</code>利用<code>New Policy</code>和<code>Old Policy</code>的比例, 限制了<code>New Policy</code>的更新幅度, 让<code>Policy Gradient</code>对稍微大点的<code>Step size</code>不那么敏感。</li></ul><h2 id="PPO整体算法"><a href="#PPO整体算法" class="headerlink" title="PPO整体算法"></a>PPO整体算法</h2><p><img src="/../../img/blogs/rl/PPO/ppo2.png" alt="1720667078392"><br><img src="/../../img/blogs/rl/PPO/ppo3.png" alt="1720667078392"></p><ul><li><code>PPO</code>是一套<code>Actor-Critic</code>结构, <code>Actor</code>想最大化<code>J_PPO</code>, <code>Critic</code>想最小化<code>L_BL</code>。<code>Critic</code>的<code>loss</code>好说, 就是减小<code>TD error</code>。而<code>Actor</code>的就是在<code>old Policy</code>上根据<code>Advantage (TD error)</code>修改<code>new Policy</code>, <code>advantage</code>大的时候, 修改幅度大, 让<code>new Policy</code>更可能发生。而且他们附加了一个<code>KL Penalty</code>(惩罚项), 简单来说, 如果<code>new Policy</code>和 <code>old Policy</code>差太多, 那<code>KL divergence</code>也越大。</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">A simple version of Proximal Policy Optimization (PPO) using single thread.</span><br><span class="hljs-string"></span><br><span class="hljs-string">Based on:</span><br><span class="hljs-string">1. Emergence of Locomotion Behaviours in Rich Environments (Google Deepmind): [https://arxiv.org/abs/1707.02286]</span><br><span class="hljs-string">2. Proximal Policy Optimization Algorithms (OpenAI): [https://arxiv.org/abs/1707.06347]</span><br><span class="hljs-string"></span><br><span class="hljs-string">View more on my tutorial website: https://morvanzhou.github.io/tutorials</span><br><span class="hljs-string"></span><br><span class="hljs-string">Dependencies:</span><br><span class="hljs-string">tensorflow r1.2</span><br><span class="hljs-string">gym 0.9.2</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> gym<br><br>EP_MAX = <span class="hljs-number">1000</span><br>EP_LEN = <span class="hljs-number">200</span><br>GAMMA = <span class="hljs-number">0.9</span><br>A_LR = <span class="hljs-number">0.0001</span><br>C_LR = <span class="hljs-number">0.0002</span><br>BATCH = <span class="hljs-number">32</span><br>A_UPDATE_STEPS = <span class="hljs-number">10</span><br>C_UPDATE_STEPS = <span class="hljs-number">10</span><br>S_DIM, A_DIM = <span class="hljs-number">3</span>, <span class="hljs-number">1</span><br>METHOD = [<br>    <span class="hljs-built_in">dict</span>(name=<span class="hljs-string">&#x27;kl_pen&#x27;</span>, kl_target=<span class="hljs-number">0.01</span>, lam=<span class="hljs-number">0.5</span>),   <span class="hljs-comment"># KL penalty</span><br>    <span class="hljs-built_in">dict</span>(name=<span class="hljs-string">&#x27;clip&#x27;</span>, epsilon=<span class="hljs-number">0.2</span>),                 <span class="hljs-comment"># Clipped surrogate objective, find this is better</span><br>][<span class="hljs-number">1</span>]        <span class="hljs-comment"># choose the method for optimization</span><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PPO</span>(<span class="hljs-title class_ inherited__">object</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.sess = tf.Session()<br>        self.tfs = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, S_DIM], <span class="hljs-string">&#x27;state&#x27;</span>)<br><br>        <span class="hljs-comment"># critic</span><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;critic&#x27;</span>):<br>            l1 = tf.layers.dense(self.tfs, <span class="hljs-number">100</span>, tf.nn.relu)<br>            self.v = tf.layers.dense(l1, <span class="hljs-number">1</span>)<br>            self.tfdc_r = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;discounted_r&#x27;</span>)<br>            self.advantage = self.tfdc_r - self.v<br>            self.closs = tf.reduce_mean(tf.square(self.advantage))<br>            self.ctrain_op = tf.train.AdamOptimizer(C_LR).minimize(self.closs)<br><br>        <span class="hljs-comment"># actor</span><br>        pi, pi_params = self._build_anet(<span class="hljs-string">&#x27;pi&#x27;</span>, trainable=<span class="hljs-literal">True</span>)<br>        oldpi, oldpi_params = self._build_anet(<span class="hljs-string">&#x27;oldpi&#x27;</span>, trainable=<span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;sample_action&#x27;</span>):<br>            self.sample_op = tf.squeeze(pi.sample(<span class="hljs-number">1</span>), axis=<span class="hljs-number">0</span>)       <span class="hljs-comment"># choosing action</span><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;update_oldpi&#x27;</span>):<br>            self.update_oldpi_op = [oldp.assign(p) <span class="hljs-keyword">for</span> p, oldp <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(pi_params, oldpi_params)]<br><br>        self.tfa = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, A_DIM], <span class="hljs-string">&#x27;action&#x27;</span>)<br>        self.tfadv = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;advantage&#x27;</span>)<br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;loss&#x27;</span>):<br>            <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;surrogate&#x27;</span>):<br>                <span class="hljs-comment"># ratio = tf.exp(pi.log_prob(self.tfa) - oldpi.log_prob(self.tfa))</span><br>                ratio = pi.prob(self.tfa) / (oldpi.prob(self.tfa) + <span class="hljs-number">1e-5</span>)<br>                surr = ratio * self.tfadv<br>            <span class="hljs-keyword">if</span> METHOD[<span class="hljs-string">&#x27;name&#x27;</span>] == <span class="hljs-string">&#x27;kl_pen&#x27;</span>:<br>                self.tflam = tf.placeholder(tf.float32, <span class="hljs-literal">None</span>, <span class="hljs-string">&#x27;lambda&#x27;</span>)<br>                kl = tf.distributions.kl_divergence(oldpi, pi)<br>                self.kl_mean = tf.reduce_mean(kl)<br>                self.aloss = -(tf.reduce_mean(surr - self.tflam * kl))<br>            <span class="hljs-keyword">else</span>:   <span class="hljs-comment"># clipping method, find this is better</span><br>                self.aloss = -tf.reduce_mean(tf.minimum(<br>                    surr,<br>                    tf.clip_by_value(ratio, <span class="hljs-number">1.</span>-METHOD[<span class="hljs-string">&#x27;epsilon&#x27;</span>], <span class="hljs-number">1.</span>+METHOD[<span class="hljs-string">&#x27;epsilon&#x27;</span>])*self.tfadv))<br><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;atrain&#x27;</span>):<br>            self.atrain_op = tf.train.AdamOptimizer(A_LR).minimize(self.aloss)<br><br>        tf.summary.FileWriter(<span class="hljs-string">&quot;log/&quot;</span>, self.sess.graph)<br><br>        self.sess.run(tf.global_variables_initializer())<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, s, a, r</span>):<br>        self.sess.run(self.update_oldpi_op)<br>        adv = self.sess.run(self.advantage, &#123;self.tfs: s, self.tfdc_r: r&#125;)<br>        <span class="hljs-comment"># adv = (adv - adv.mean())/(adv.std()+1e-6)     # sometimes helpful</span><br><br>        <span class="hljs-comment"># update actor</span><br>        <span class="hljs-keyword">if</span> METHOD[<span class="hljs-string">&#x27;name&#x27;</span>] == <span class="hljs-string">&#x27;kl_pen&#x27;</span>:<br>            <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(A_UPDATE_STEPS):<br>                _, kl = self.sess.run(<br>                    [self.atrain_op, self.kl_mean],<br>                    &#123;self.tfs: s, self.tfa: a, self.tfadv: adv, self.tflam: METHOD[<span class="hljs-string">&#x27;lam&#x27;</span>]&#125;)<br>                <span class="hljs-keyword">if</span> kl &gt; <span class="hljs-number">4</span>*METHOD[<span class="hljs-string">&#x27;kl_target&#x27;</span>]:  <span class="hljs-comment"># this in in google&#x27;s paper</span><br>                    <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">if</span> kl &lt; METHOD[<span class="hljs-string">&#x27;kl_target&#x27;</span>] / <span class="hljs-number">1.5</span>:  <span class="hljs-comment"># adaptive lambda, this is in OpenAI&#x27;s paper</span><br>                METHOD[<span class="hljs-string">&#x27;lam&#x27;</span>] /= <span class="hljs-number">2</span><br>            <span class="hljs-keyword">elif</span> kl &gt; METHOD[<span class="hljs-string">&#x27;kl_target&#x27;</span>] * <span class="hljs-number">1.5</span>:<br>                METHOD[<span class="hljs-string">&#x27;lam&#x27;</span>] *= <span class="hljs-number">2</span><br>            METHOD[<span class="hljs-string">&#x27;lam&#x27;</span>] = np.clip(METHOD[<span class="hljs-string">&#x27;lam&#x27;</span>], <span class="hljs-number">1e-4</span>, <span class="hljs-number">10</span>)    <span class="hljs-comment"># sometimes explode, this clipping is my solution</span><br>        <span class="hljs-keyword">else</span>:   <span class="hljs-comment"># clipping method, find this is better (OpenAI&#x27;s paper)</span><br>            [self.sess.run(self.atrain_op, &#123;self.tfs: s, self.tfa: a, self.tfadv: adv&#125;) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(A_UPDATE_STEPS)]<br><br>        <span class="hljs-comment"># update critic</span><br>        [self.sess.run(self.ctrain_op, &#123;self.tfs: s, self.tfdc_r: r&#125;) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(C_UPDATE_STEPS)]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_anet</span>(<span class="hljs-params">self, name, trainable</span>):<br>        <span class="hljs-keyword">with</span> tf.variable_scope(name):<br>            l1 = tf.layers.dense(self.tfs, <span class="hljs-number">100</span>, tf.nn.relu, trainable=trainable)<br>            mu = <span class="hljs-number">2</span> * tf.layers.dense(l1, A_DIM, tf.nn.tanh, trainable=trainable)<br>            sigma = tf.layers.dense(l1, A_DIM, tf.nn.softplus, trainable=trainable)<br>            norm_dist = tf.distributions.Normal(loc=mu, scale=sigma)<br>        params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)<br>        <span class="hljs-keyword">return</span> norm_dist, params<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">self, s</span>):<br>        s = s[np.newaxis, :]<br>        a = self.sess.run(self.sample_op, &#123;self.tfs: s&#125;)[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">return</span> np.clip(a, -<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_v</span>(<span class="hljs-params">self, s</span>):<br>        <span class="hljs-keyword">if</span> s.ndim &lt; <span class="hljs-number">2</span>: s = s[np.newaxis, :]<br>        <span class="hljs-keyword">return</span> self.sess.run(self.v, &#123;self.tfs: s&#125;)[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>]<br><br>env = gym.make(<span class="hljs-string">&#x27;Pendulum-v0&#x27;</span>).unwrapped<br>ppo = PPO()<br>all_ep_r = []<br><br><span class="hljs-keyword">for</span> ep <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EP_MAX):<br>    s = env.reset()<br>    buffer_s, buffer_a, buffer_r = [], [], []<br>    ep_r = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EP_LEN):    <span class="hljs-comment"># in one episode</span><br>        env.render()<br>        a = ppo.choose_action(s)<br>        s_, r, done, _ = env.step(a)<br>        buffer_s.append(s)<br>        buffer_a.append(a)<br>        buffer_r.append((r+<span class="hljs-number">8</span>)/<span class="hljs-number">8</span>)    <span class="hljs-comment"># normalize reward, find to be useful</span><br>        s = s_<br>        ep_r += r<br><br>        <span class="hljs-comment"># update ppo</span><br>        <span class="hljs-keyword">if</span> (t+<span class="hljs-number">1</span>) % BATCH == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> t == EP_LEN-<span class="hljs-number">1</span>:<br>            v_s_ = ppo.get_v(s_)<br>            discounted_r = []<br>            <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> buffer_r[::-<span class="hljs-number">1</span>]:<br>                v_s_ = r + GAMMA * v_s_<br>                discounted_r.append(v_s_)<br>            discounted_r.reverse()<br><br>            bs, ba, br = np.vstack(buffer_s), np.vstack(buffer_a), np.array(discounted_r)[:, np.newaxis]<br>            buffer_s, buffer_a, buffer_r = [], [], []<br>            ppo.update(bs, ba, br)<br>    <span class="hljs-keyword">if</span> ep == <span class="hljs-number">0</span>: all_ep_r.append(ep_r)<br>    <span class="hljs-keyword">else</span>: all_ep_r.append(all_ep_r[-<span class="hljs-number">1</span>]*<span class="hljs-number">0.9</span> + ep_r*<span class="hljs-number">0.1</span>)<br>    <span class="hljs-built_in">print</span>(<br>        <span class="hljs-string">&#x27;Ep: %i&#x27;</span> % ep,<br>        <span class="hljs-string">&quot;|Ep_r: %i&quot;</span> % ep_r,<br>        (<span class="hljs-string">&quot;|Lam: %.4f&quot;</span> % METHOD[<span class="hljs-string">&#x27;lam&#x27;</span>]) <span class="hljs-keyword">if</span> METHOD[<span class="hljs-string">&#x27;name&#x27;</span>] == <span class="hljs-string">&#x27;kl_pen&#x27;</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;&#x27;</span>,<br>    )<br><br>plt.plot(np.arange(<span class="hljs-built_in">len</span>(all_ep_r)), all_ep_r)<br>plt.xlabel(<span class="hljs-string">&#x27;Episode&#x27;</span>);plt.ylabel(<span class="hljs-string">&#x27;Moving averaged episode reward&#x27;</span>);plt.show()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>RL算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Actor Critic</title>
    <link href="/2024/08/05/rl/5-Actor%20Critic/"/>
    <url>/2024/08/05/rl/5-Actor%20Critic/</url>
    
    <content type="html"><![CDATA[<h1 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor Critic"></a>Actor Critic</h1><p><img src="/../../img/blogs/rl/AC/AC2.png" alt="1720667162152"></p><ul><li>结合了<code>Policy Gradient (Actor)</code>和<code>Function Approximation (Critic)</code>的方法。<code>Actor</code>基于概率选行为,<code>Critic</code>基于<code>Actor</code>的行为评判行为的得分,<code>Actor</code>根据<code>Critic</code>的评分修改选行为的概率。</li></ul><h2 id="Actor-Critic优势"><a href="#Actor-Critic优势" class="headerlink" title="Actor Critic优势"></a>Actor Critic优势</h2><ul><li>可以进行单步更新, 比传统的 Policy Gradient 要快。</li></ul><h2 id="Actor-Critic劣势"><a href="#Actor-Critic劣势" class="headerlink" title="Actor Critic劣势"></a>Actor Critic劣势</h2><ul><li>取决于 Critic 的价值判断, 但是 Critic 难收敛, 再加上 Actor 的更新, 就更难收敛.</li></ul><h2 id="Actor-Critic整体算法"><a href="#Actor-Critic整体算法" class="headerlink" title="Actor Critic整体算法"></a>Actor Critic整体算法</h2><p><img src="/../../img/blogs/rl/AC/AC4.png" alt="1720667162152"><br><img src="/../../img/blogs/rl/AC/AC5.png" alt="1720667162152"></p><ul><li>Actor 在运用 Policy Gradient 的方法进行 Gradient ascent 的时候, 由 Critic 来告诉他, 这次的 Gradient ascent 是不是一次正确的 ascent, 如果这次的得分不好, 那么就不要 ascent 那么多。</li></ul>]]></content>
    
    
    <categories>
      
      <category>RL算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DQN</title>
    <link href="/2024/08/05/rl/3-DQN/"/>
    <url>/2024/08/05/rl/3-DQN/</url>
    
    <content type="html"><![CDATA[<h1 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h1><h2 id="神经网络的作用"><a href="#神经网络的作用" class="headerlink" title="神经网络的作用"></a>神经网络的作用</h2><p><img src="/../../img/blogs/rl/DQN/DQN2.png" alt="1720667078392"></p><ul><li>方式1：将状态和动作当成神经网络的输入, 然后经过神经网络分析后得到动作的 Q 值。</li><li>方式2：只输入状态值, 输出所有的动作值, 然后按照 Q-learning 的原则, 直接选择拥有最大值的动作当做下一步要做的动作。</li></ul><h2 id="神经网络更新"><a href="#神经网络更新" class="headerlink" title="神经网络更新"></a>神经网络更新</h2><p><img src="/../../img/blogs/rl/DQN/DQN3.png" alt="1720667162152"><br><img src="/../../img/blogs/rl/DQN/DQN4.png" alt="1720667162152"></p><ul><li><strong>Q估计</strong>：通过<code>NN</code>预测出<code>Q(s2, a1)</code> 和<code>Q(s2,a2)</code>的值，然后选取<code>Q估计</code>中最大值的动作来换取环境中的奖励<code>reward</code>。 </li><li><strong>Q现实</strong>：通过神经网络分析下一步在<code>s&#39;</code>的两个<code>Q估计</code>值</li><li><strong>更新</strong>：神经网络的的参数就是<code>老的NN参数 + 学习率alpha × (Q现实 - Q估计)</code>。</li></ul><h2 id="DQN-两大利器"><a href="#DQN-两大利器" class="headerlink" title="DQN 两大利器"></a>DQN 两大利器</h2><p><img src="/../img/blogs/rl/DQN/DQN5.png" alt="1720667162152"><br><img src="/../img/blogs/rl/DQN/DQN6.png" alt="1720667162152"></p><ul><li><strong>Experience Replay</strong>：<code>DQN</code>有一个记忆库用于学习之前的经历，每次<code>DQN</code>更新的时候, 可以随机抽取一些之前的经历进行学习。 随机抽取这种做法打乱了经历之间的相关性, 也使得神经网络更新更有效率。(off-policy)</li><li><strong>Fixed Q-targets</strong>：在<code>DQN</code>中使用到两个结构相同但参数不同的神经网络, 预测<code>Q 估计</code>的神经网络具备最新的参数, 而预测<code>Q 现实</code>的神经网络使用的参数则是很久以前的。</li></ul><h2 id="算法伪代码"><a href="#算法伪代码" class="headerlink" title="算法伪代码"></a>算法伪代码</h2><p><img src="/../../img/blogs/rl/DQN/DQN6.jpg" alt="1720667162152"></p><p>对比Q-learning：</p><ul><li>记忆库 (用于重复学习)</li><li>神经网络计算<code>Q 值</code></li><li>暂时冻结<code>q_target参数</code> (切断相关性)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">This part of code is the Q learning brain, which is a brain of the agent.</span><br><span class="hljs-string">All decisions are made in here.</span><br><span class="hljs-string"></span><br><span class="hljs-string">View more on my tutorial page: https://morvanzhou.github.io/tutorials/</span><br><span class="hljs-string"></span><br><span class="hljs-string">Pytorch: https://github.com/ClownW/Reinforcement-learning-with-PyTorch</span><br><span class="hljs-string">Tensorflow: https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><br>np.random.seed(<span class="hljs-number">1</span>)<br>tf.set_random_seed(<span class="hljs-number">1</span>)<br><br><br><span class="hljs-comment"># Deep Q Network off-policy</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DeepQNetwork</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">            self,</span><br><span class="hljs-params">            n_actions,</span><br><span class="hljs-params">            n_features,</span><br><span class="hljs-params">            learning_rate=<span class="hljs-number">0.01</span>,</span><br><span class="hljs-params">            reward_decay=<span class="hljs-number">0.9</span>,</span><br><span class="hljs-params">            e_greedy=<span class="hljs-number">0.9</span>,</span><br><span class="hljs-params">            replace_target_iter=<span class="hljs-number">300</span>,</span><br><span class="hljs-params">            memory_size=<span class="hljs-number">500</span>,</span><br><span class="hljs-params">            batch_size=<span class="hljs-number">32</span>,</span><br><span class="hljs-params">            e_greedy_increment=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">            output_graph=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    </span>):<br>        self.n_actions = n_actions<br>        self.n_features = n_features<br>        self.lr = learning_rate<br>        self.gamma = reward_decay<br>        self.epsilon_max = e_greedy<br>        self.replace_target_iter = replace_target_iter<br>        self.memory_size = memory_size<br>        self.batch_size = batch_size<br>        self.epsilon_increment = e_greedy_increment<br>        self.epsilon = <span class="hljs-number">0</span> <span class="hljs-keyword">if</span> e_greedy_increment <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> self.epsilon_max<br><br>        <span class="hljs-comment"># total learning step</span><br>        self.learn_step_counter = <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># initialize zero memory [s, a, r, s_]</span><br>        self.memory = np.zeros((self.memory_size, n_features * <span class="hljs-number">2</span> + <span class="hljs-number">2</span>))<br><br>        <span class="hljs-comment"># consist of [target_net, evaluate_net]</span><br>        self._build_net()<br><br>        t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="hljs-string">&#x27;target_net&#x27;</span>)<br>        e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="hljs-string">&#x27;eval_net&#x27;</span>)<br><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;hard_replacement&#x27;</span>):<br>            self.target_replace_op = [tf.assign(t, e) <span class="hljs-keyword">for</span> t, e <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(t_params, e_params)]<br><br>        self.sess = tf.Session()<br><br>        <span class="hljs-keyword">if</span> output_graph:<br>            <span class="hljs-comment"># $ tensorboard --logdir=logs</span><br>            tf.summary.FileWriter(<span class="hljs-string">&quot;logs/&quot;</span>, self.sess.graph)<br><br>        self.sess.run(tf.global_variables_initializer())<br>        self.cost_his = []<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_net</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># ------------------ all inputs ------------------------</span><br>        self.s = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, self.n_features], name=<span class="hljs-string">&#x27;s&#x27;</span>)  <span class="hljs-comment"># input State</span><br>        self.s_ = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, self.n_features], name=<span class="hljs-string">&#x27;s_&#x27;</span>)  <span class="hljs-comment"># input Next State</span><br>        self.r = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, ], name=<span class="hljs-string">&#x27;r&#x27;</span>)  <span class="hljs-comment"># input Reward</span><br>        self.a = tf.placeholder(tf.int32, [<span class="hljs-literal">None</span>, ], name=<span class="hljs-string">&#x27;a&#x27;</span>)  <span class="hljs-comment"># input Action</span><br><br>        w_initializer, b_initializer = tf.random_normal_initializer(<span class="hljs-number">0.</span>, <span class="hljs-number">0.3</span>), tf.constant_initializer(<span class="hljs-number">0.1</span>)<br><br>        <span class="hljs-comment"># ------------------ build evaluate_net ------------------</span><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;eval_net&#x27;</span>):<br>            e1 = tf.layers.dense(self.s, <span class="hljs-number">20</span>, tf.nn.relu, kernel_initializer=w_initializer,<br>                                 bias_initializer=b_initializer, name=<span class="hljs-string">&#x27;e1&#x27;</span>)<br>            self.q_eval = tf.layers.dense(e1, self.n_actions, kernel_initializer=w_initializer,<br>                                          bias_initializer=b_initializer, name=<span class="hljs-string">&#x27;q&#x27;</span>)<br><br>        <span class="hljs-comment"># ------------------ build target_net ------------------</span><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;target_net&#x27;</span>):<br>            t1 = tf.layers.dense(self.s_, <span class="hljs-number">20</span>, tf.nn.relu, kernel_initializer=w_initializer,<br>                                 bias_initializer=b_initializer, name=<span class="hljs-string">&#x27;t1&#x27;</span>)<br>            self.q_next = tf.layers.dense(t1, self.n_actions, kernel_initializer=w_initializer,<br>                                          bias_initializer=b_initializer, name=<span class="hljs-string">&#x27;t2&#x27;</span>)<br><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;q_target&#x27;</span>):<br>            q_target = self.r + self.gamma * tf.reduce_max(self.q_next, axis=<span class="hljs-number">1</span>, name=<span class="hljs-string">&#x27;Qmax_s_&#x27;</span>)    <span class="hljs-comment"># shape=(None, )</span><br>            self.q_target = tf.stop_gradient(q_target)<br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;q_eval&#x27;</span>):<br>            a_indices = tf.stack([tf.<span class="hljs-built_in">range</span>(tf.shape(self.a)[<span class="hljs-number">0</span>], dtype=tf.int32), self.a], axis=<span class="hljs-number">1</span>)<br>            self.q_eval_wrt_a = tf.gather_nd(params=self.q_eval, indices=a_indices)    <span class="hljs-comment"># shape=(None, )</span><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;loss&#x27;</span>):<br>            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval_wrt_a, name=<span class="hljs-string">&#x27;TD_error&#x27;</span>))<br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;train&#x27;</span>):<br>            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">store_transition</span>(<span class="hljs-params">self, s, a, r, s_</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(self, <span class="hljs-string">&#x27;memory_counter&#x27;</span>):<br>            self.memory_counter = <span class="hljs-number">0</span><br>        transition = np.hstack((s, [a, r], s_))<br>        <span class="hljs-comment"># replace the old memory with new memory</span><br>        index = self.memory_counter % self.memory_size<br>        self.memory[index, :] = transition<br>        self.memory_counter += <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">self, observation</span>):<br>        <span class="hljs-comment"># to have batch dimension when feed into tf placeholder</span><br>        observation = observation[np.newaxis, :]<br><br>        <span class="hljs-keyword">if</span> np.random.uniform() &lt; self.epsilon:<br>            <span class="hljs-comment"># forward feed the observation and get q value for every actions</span><br>            actions_value = self.sess.run(self.q_eval, feed_dict=&#123;self.s: observation&#125;)<br>            action = np.argmax(actions_value)<br>        <span class="hljs-keyword">else</span>:<br>            action = np.random.randint(<span class="hljs-number">0</span>, self.n_actions)<br>        <span class="hljs-keyword">return</span> action<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># check to replace target parameters</span><br>        <span class="hljs-keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="hljs-number">0</span>:<br>            self.sess.run(self.target_replace_op)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\ntarget_params_replaced\n&#x27;</span>)<br><br>        <span class="hljs-comment"># sample batch memory from all memory</span><br>        <span class="hljs-keyword">if</span> self.memory_counter &gt; self.memory_size:<br>            sample_index = np.random.choice(self.memory_size, size=self.batch_size)<br>        <span class="hljs-keyword">else</span>:<br>            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)<br>        batch_memory = self.memory[sample_index, :]<br><br>        _, cost = self.sess.run(<br>            [self._train_op, self.loss],<br>            feed_dict=&#123;<br>                self.s: batch_memory[:, :self.n_features],<br>                self.a: batch_memory[:, self.n_features],<br>                self.r: batch_memory[:, self.n_features + <span class="hljs-number">1</span>],<br>                self.s_: batch_memory[:, -self.n_features:],<br>            &#125;)<br><br>        self.cost_his.append(cost)<br><br>        <span class="hljs-comment"># increasing epsilon</span><br>        self.epsilon = self.epsilon + self.epsilon_increment <span class="hljs-keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="hljs-keyword">else</span> self.epsilon_max<br>        self.learn_step_counter += <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_cost</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>        plt.plot(np.arange(<span class="hljs-built_in">len</span>(self.cost_his)), self.cost_his)<br>        plt.ylabel(<span class="hljs-string">&#x27;Cost&#x27;</span>)<br>        plt.xlabel(<span class="hljs-string">&#x27;training steps&#x27;</span>)<br>        plt.show()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    DQN = DeepQNetwork(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>, output_graph=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h1 id="Double-DQN"><a href="#Double-DQN" class="headerlink" title="Double DQN"></a>Double DQN</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ul><li>在实际问题中, 如果你输出你的<code>DQN</code>的<code>Q 值</code>, 可能就会发现, <code>Q 值</code>都超级大。这是由于DQN 基于<code>Q-learning</code>, 其中<code>Qmax</code>会导致<code>Q现实</code>当中的过估计 (overestimate)。 而<code>Double DQN</code>就是用来解决过估计的。</li></ul><h2 id="Double-DQN-算法"><a href="#Double-DQN-算法" class="headerlink" title="Double DQN 算法"></a>Double DQN 算法</h2><ul><li>两个神经网络: Q_eval (Q估计中的), Q_next (Q现实中的)。</li><li>原本的<code>Q_next = max(Q_next(s&#39;, a_all))</code>。</li><li>Double DQN 中的<code>Q_next = Q_next(s&#39;, argmax(Q_eval(s&#39;, a_all)))</code>。 也可以表达成下面的式子：</li></ul><p><img src="/../../img/blogs/rl/DQN/DQN7.png" alt="1720667162152"></p>]]></content>
    
    
    <categories>
      
      <category>RL算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DDPG</title>
    <link href="/2024/08/05/rl/6-DDPG/"/>
    <url>/2024/08/05/rl/6-DDPG/</url>
    
    <content type="html"><![CDATA[<h1 id="Deep-Deterministic-Policy-Gradient（DDPG）"><a href="#Deep-Deterministic-Policy-Gradient（DDPG）" class="headerlink" title="Deep Deterministic Policy Gradient（DDPG）"></a>Deep Deterministic Policy Gradient（DDPG）</h1><p><img src="/../../img/blogs/rl/DDPG/DDPG4.png" alt="1720667162152"></p><ul><li>Google DeepMind 提出的一种使用 Actor Critic 结构, 但是输出的不是行为的概率, 而是具体的行为, 用于连续动作 (continuous action) 的预测。 DDPG 结合了之前获得成功的 DQN 结构, 提高了 Actor Critic 的稳定性和收敛性。</li></ul><h2 id="DDPG更新"><a href="#DDPG更新" class="headerlink" title="DDPG更新"></a>DDPG更新</h2><ul><li><p>**<code>Actor</code>**：它的前半部分<code>grad[Q]</code>是从<code>Critic</code>来的, 这是在说: 这次<code>Actor</code>的动作要怎么移动, 才能获得更大的<code>Q</code>, 而后半部分<code>grad[μ]</code>是从<code>Actor</code>来的, 这是在说:<code>Actor</code>要怎么样修改自身参数, 使得<code>Actor</code>更有可能做这个动作。 所以两者合起来就是在说:<code>Actor</code>要朝着更有可能获取大<code>Q</code>的方向修改动作参数了。<br><img src="/../../img/blogs/rl/DDPG/DDPG5.png" alt="1720667162152"></p></li><li><p>**<code>Critic</code>**：借鉴了<code>DQN</code>和<code>Double Q learning</code>的方式, 有两个计算<code>Q</code>的神经网络, <code>Q_target</code>中依据下一状态, 用<code>Actor</code>来选择动作, 而这时的<code>Actor</code>也是一个<code>Actor_target</code>(有着 Actor 很久之前的参数)。 使用这种方法获得的<code>Q_target</code>能像<code>DQN</code>那样切断相关性, 提高收敛性。<br><img src="/../../img/blogs/rl/DDPG/DDPG6.png" alt="1720667162152"></p></li></ul><h2 id="DDPG整体算法"><a href="#DDPG整体算法" class="headerlink" title="DDPG整体算法"></a>DDPG整体算法</h2><p><img src="/../../img/blogs/rl/DDPG/DDPG7.png" alt="1720667162152"></p><ul><li>Actor 在运用 Policy Gradient 的方法进行 Gradient ascent 的时候, 由 Critic 来告诉他, 这次的 Gradient ascent 是不是一次正确的 ascent, 如果这次的得分不好, 那么就不要 ascent 那么多。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Note: This is a updated version from my previous code,</span><br><span class="hljs-string">for the target network, I use moving average to soft replace target parameters instead using assign function.</span><br><span class="hljs-string">By doing this, it has 20% speed up on my machine (CPU).</span><br><span class="hljs-string"></span><br><span class="hljs-string">Deep Deterministic Policy Gradient (DDPG), Reinforcement Learning.</span><br><span class="hljs-string">DDPG is Actor Critic based algorithm.</span><br><span class="hljs-string">Pendulum example.</span><br><span class="hljs-string"></span><br><span class="hljs-string">View more on my tutorial page: https://morvanzhou.github.io/tutorials/</span><br><span class="hljs-string"></span><br><span class="hljs-string">Using:</span><br><span class="hljs-string">tensorflow 1.0</span><br><span class="hljs-string">gym 0.8.0</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> gym<br><span class="hljs-keyword">import</span> time<br><br><br><span class="hljs-comment">#####################  hyper parameters  ####################</span><br><br>MAX_EPISODES = <span class="hljs-number">200</span><br>MAX_EP_STEPS = <span class="hljs-number">200</span><br>LR_A = <span class="hljs-number">0.001</span>    <span class="hljs-comment"># learning rate for actor</span><br>LR_C = <span class="hljs-number">0.002</span>    <span class="hljs-comment"># learning rate for critic</span><br>GAMMA = <span class="hljs-number">0.9</span>     <span class="hljs-comment"># reward discount</span><br>TAU = <span class="hljs-number">0.01</span>      <span class="hljs-comment"># soft replacement</span><br>MEMORY_CAPACITY = <span class="hljs-number">10000</span><br>BATCH_SIZE = <span class="hljs-number">32</span><br><br>RENDER = <span class="hljs-literal">False</span><br>ENV_NAME = <span class="hljs-string">&#x27;Pendulum-v0&#x27;</span><br><br><br><span class="hljs-comment">###############################  DDPG  ####################################</span><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DDPG</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, a_dim, s_dim, a_bound,</span>):<br>        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * <span class="hljs-number">2</span> + a_dim + <span class="hljs-number">1</span>), dtype=np.float32)<br>        self.pointer = <span class="hljs-number">0</span><br>        self.sess = tf.Session()<br><br>        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound,<br>        self.S = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, s_dim], <span class="hljs-string">&#x27;s&#x27;</span>)<br>        self.S_ = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, s_dim], <span class="hljs-string">&#x27;s_&#x27;</span>)<br>        self.R = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;r&#x27;</span>)<br><br>        self.a = self._build_a(self.S,)<br>        q = self._build_c(self.S, self.a, )<br>        a_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=<span class="hljs-string">&#x27;Actor&#x27;</span>)<br>        c_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=<span class="hljs-string">&#x27;Critic&#x27;</span>)<br>        ema = tf.train.ExponentialMovingAverage(decay=<span class="hljs-number">1</span> - TAU)          <span class="hljs-comment"># soft replacement</span><br><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">ema_getter</span>(<span class="hljs-params">getter, name, *args, **kwargs</span>):<br>            <span class="hljs-keyword">return</span> ema.average(getter(name, *args, **kwargs))<br><br>        target_update = [ema.apply(a_params), ema.apply(c_params)]      <span class="hljs-comment"># soft update operation</span><br>        a_ = self._build_a(self.S_, reuse=<span class="hljs-literal">True</span>, custom_getter=ema_getter)   <span class="hljs-comment"># replaced target parameters</span><br>        q_ = self._build_c(self.S_, a_, reuse=<span class="hljs-literal">True</span>, custom_getter=ema_getter)<br><br>        a_loss = - tf.reduce_mean(q)  <span class="hljs-comment"># maximize the q</span><br>        self.atrain = tf.train.AdamOptimizer(LR_A).minimize(a_loss, var_list=a_params)<br><br>        <span class="hljs-keyword">with</span> tf.control_dependencies(target_update):    <span class="hljs-comment"># soft replacement happened at here</span><br>            q_target = self.R + GAMMA * q_<br>            td_error = tf.losses.mean_squared_error(labels=q_target, predictions=q)<br>            self.ctrain = tf.train.AdamOptimizer(LR_C).minimize(td_error, var_list=c_params)<br><br>        self.sess.run(tf.global_variables_initializer())<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">self, s</span>):<br>        <span class="hljs-keyword">return</span> self.sess.run(self.a, &#123;self.S: s[np.newaxis, :]&#125;)[<span class="hljs-number">0</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self</span>):<br>        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)<br>        bt = self.memory[indices, :]<br>        bs = bt[:, :self.s_dim]<br>        ba = bt[:, self.s_dim: self.s_dim + self.a_dim]<br>        br = bt[:, -self.s_dim - <span class="hljs-number">1</span>: -self.s_dim]<br>        bs_ = bt[:, -self.s_dim:]<br><br>        self.sess.run(self.atrain, &#123;self.S: bs&#125;)<br>        self.sess.run(self.ctrain, &#123;self.S: bs, self.a: ba, self.R: br, self.S_: bs_&#125;)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">store_transition</span>(<span class="hljs-params">self, s, a, r, s_</span>):<br>        transition = np.hstack((s, a, [r], s_))<br>        index = self.pointer % MEMORY_CAPACITY  <span class="hljs-comment"># replace the old memory with new memory</span><br>        self.memory[index, :] = transition<br>        self.pointer += <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_a</span>(<span class="hljs-params">self, s, reuse=<span class="hljs-literal">None</span>, custom_getter=<span class="hljs-literal">None</span></span>):<br>        trainable = <span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> reuse <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;Actor&#x27;</span>, reuse=reuse, custom_getter=custom_getter):<br>            net = tf.layers.dense(s, <span class="hljs-number">30</span>, activation=tf.nn.relu, name=<span class="hljs-string">&#x27;l1&#x27;</span>, trainable=trainable)<br>            a = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, name=<span class="hljs-string">&#x27;a&#x27;</span>, trainable=trainable)<br>            <span class="hljs-keyword">return</span> tf.multiply(a, self.a_bound, name=<span class="hljs-string">&#x27;scaled_a&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_c</span>(<span class="hljs-params">self, s, a, reuse=<span class="hljs-literal">None</span>, custom_getter=<span class="hljs-literal">None</span></span>):<br>        trainable = <span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> reuse <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;Critic&#x27;</span>, reuse=reuse, custom_getter=custom_getter):<br>            n_l1 = <span class="hljs-number">30</span><br>            w1_s = tf.get_variable(<span class="hljs-string">&#x27;w1_s&#x27;</span>, [self.s_dim, n_l1], trainable=trainable)<br>            w1_a = tf.get_variable(<span class="hljs-string">&#x27;w1_a&#x27;</span>, [self.a_dim, n_l1], trainable=trainable)<br>            b1 = tf.get_variable(<span class="hljs-string">&#x27;b1&#x27;</span>, [<span class="hljs-number">1</span>, n_l1], trainable=trainable)<br>            net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)<br>            <span class="hljs-keyword">return</span> tf.layers.dense(net, <span class="hljs-number">1</span>, trainable=trainable)  <span class="hljs-comment"># Q(s,a)</span><br><br><br><span class="hljs-comment">###############################  training  ####################################</span><br><br>env = gym.make(ENV_NAME)<br>env = env.unwrapped<br>env.seed(<span class="hljs-number">1</span>)<br><br>s_dim = env.observation_space.shape[<span class="hljs-number">0</span>]<br>a_dim = env.action_space.shape[<span class="hljs-number">0</span>]<br>a_bound = env.action_space.high<br><br>ddpg = DDPG(a_dim, s_dim, a_bound)<br><br>var = <span class="hljs-number">3</span>  <span class="hljs-comment"># control exploration</span><br>t1 = time.time()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(MAX_EPISODES):<br>    s = env.reset()<br>    ep_reward = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(MAX_EP_STEPS):<br>        <span class="hljs-keyword">if</span> RENDER:<br>            env.render()<br><br>        <span class="hljs-comment"># Add exploration noise</span><br>        a = ddpg.choose_action(s)<br>        a = np.clip(np.random.normal(a, var), -<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)    <span class="hljs-comment"># add randomness to action selection for exploration</span><br>        s_, r, done, info = env.step(a)<br><br>        ddpg.store_transition(s, a, r / <span class="hljs-number">10</span>, s_)<br><br>        <span class="hljs-keyword">if</span> ddpg.pointer &gt; MEMORY_CAPACITY:<br>            var *= <span class="hljs-number">.9995</span>    <span class="hljs-comment"># decay the action randomness</span><br>            ddpg.learn()<br><br>        s = s_<br>        ep_reward += r<br>        <span class="hljs-keyword">if</span> j == MAX_EP_STEPS-<span class="hljs-number">1</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Episode:&#x27;</span>, i, <span class="hljs-string">&#x27; Reward: %i&#x27;</span> % <span class="hljs-built_in">int</span>(ep_reward), <span class="hljs-string">&#x27;Explore: %.2f&#x27;</span> % var, )<br>            <span class="hljs-comment"># if ep_reward &gt; -300:RENDER = True</span><br>            <span class="hljs-keyword">break</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Running time: &#x27;</span>, time.time() - t1)<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>RL算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Sarsa</title>
    <link href="/2024/07/12/rl/2-Sarsa/"/>
    <url>/2024/07/12/rl/2-Sarsa/</url>
    
    <content type="html"><![CDATA[<h1 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h1><h2 id="Sarsa-决策"><a href="#Sarsa-决策" class="headerlink" title="Sarsa 决策"></a>Sarsa 决策</h2><p><img src="/../../img/blogs/rl/Sarsa/s2.png" alt="1720667078392"></p><ul><li>学习完成后，根据当前状态在Q值表中的<em><strong>最大</strong></em>Q值来选取动作</li></ul><h2 id="Sarsa更新"><a href="#Sarsa更新" class="headerlink" title="Sarsa更新"></a>Sarsa更新</h2><p><img src="/../../img/blogs/rl/Sarsa/s3.png" alt="1720667162152"></p><ul><li><strong>更新Q值表</strong>：通过计算现实Q值和估计Q值的差距来更新</li><li><strong>现实Q值</strong>：估算的动作也是接下来要做的动作（on-policy）</li><li><strong>估计Q值</strong>：原Q值表中对应的Q值</li></ul><h2 id="Sarsa整体算法"><a href="#Sarsa整体算法" class="headerlink" title="Sarsa整体算法"></a>Sarsa整体算法</h2><p><img src="/../../img/blogs/rl/Sarsa/s4.png" alt="1720667162152"></p><ul><li><strong>不同之处</strong>：Sarsa是说到做到型，所以称为on-policy，在线学习，学着自己在做的事情。而Q-learning 是说到但并不一定做到，所以称为Off-policy，离线学习。</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">This part of code is the Q learning brain, which is a brain of the agent.</span><br><span class="hljs-string">All decisions are made in here.</span><br><span class="hljs-string"></span><br><span class="hljs-string">View more on my tutorial page: https://morvanzhou.github.io/tutorials/</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RL</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, action_space, learning_rate=<span class="hljs-number">0.01</span>, reward_decay=<span class="hljs-number">0.9</span>, e_greedy=<span class="hljs-number">0.9</span></span>):<br>        self.actions = action_space  <span class="hljs-comment"># a list</span><br>        self.lr = learning_rate<br>        self.gamma = reward_decay<br>        self.epsilon = e_greedy<br><br>        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">check_state_exist</span>(<span class="hljs-params">self, state</span>):<br>        <span class="hljs-keyword">if</span> state <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.q_table.index:<br>            <span class="hljs-comment"># append new state to q table</span><br>            self.q_table = self.q_table.append(<br>                pd.Series(<br>                    [<span class="hljs-number">0</span>]*<span class="hljs-built_in">len</span>(self.actions),<br>                    index=self.q_table.columns,<br>                    name=state,<br>                )<br>            )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">self, observation</span>):<br>        self.check_state_exist(observation)<br>        <span class="hljs-comment"># action selection</span><br>        <span class="hljs-keyword">if</span> np.random.rand() &lt; self.epsilon:<br>            <span class="hljs-comment"># choose best action</span><br>            state_action = self.q_table.loc[observation, :]<br>            <span class="hljs-comment"># some actions may have the same value, randomly choose on in these actions</span><br>            action = np.random.choice(state_action[state_action == np.<span class="hljs-built_in">max</span>(state_action)].index)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># choose random action</span><br>            action = np.random.choice(self.actions)<br>        <span class="hljs-keyword">return</span> action<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self, *args</span>):<br>        <span class="hljs-keyword">pass</span><br><br><br><span class="hljs-comment"># off-policy</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">QLearningTable</span>(<span class="hljs-title class_ inherited__">RL</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, actions, learning_rate=<span class="hljs-number">0.01</span>, reward_decay=<span class="hljs-number">0.9</span>, e_greedy=<span class="hljs-number">0.9</span></span>):<br>        <span class="hljs-built_in">super</span>(QLearningTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self, s, a, r, s_</span>):<br>        self.check_state_exist(s_)<br>        q_predict = self.q_table.loc[s, a]<br>        <span class="hljs-keyword">if</span> s_ != <span class="hljs-string">&#x27;terminal&#x27;</span>:<br>            q_target = r + self.gamma * self.q_table.loc[s_, :].<span class="hljs-built_in">max</span>()  <span class="hljs-comment"># next state is not terminal</span><br>        <span class="hljs-keyword">else</span>:<br>            q_target = r  <span class="hljs-comment"># next state is terminal</span><br>        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  <span class="hljs-comment"># update</span><br><br><br><span class="hljs-comment"># on-policy</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SarsaTable</span>(<span class="hljs-title class_ inherited__">RL</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, actions, learning_rate=<span class="hljs-number">0.01</span>, reward_decay=<span class="hljs-number">0.9</span>, e_greedy=<span class="hljs-number">0.9</span></span>):<br>        <span class="hljs-built_in">super</span>(SarsaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self, s, a, r, s_, a_</span>):<br>        self.check_state_exist(s_)<br>        q_predict = self.q_table.loc[s, a]<br>        <span class="hljs-keyword">if</span> s_ != <span class="hljs-string">&#x27;terminal&#x27;</span>:<br>            q_target = r + self.gamma * self.q_table.loc[s_, a_]  <span class="hljs-comment"># next state is not terminal</span><br>        <span class="hljs-keyword">else</span>:<br>            q_target = r  <span class="hljs-comment"># next state is terminal</span><br>        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  <span class="hljs-comment"># update</span><br></code></pre></td></tr></table></figure><h1 id="Sarsa-λ"><a href="#Sarsa-λ" class="headerlink" title="Sarsa(λ)"></a>Sarsa(λ)</h1><h2 id="单步更新-and-回合更新"><a href="#单步更新-and-回合更新" class="headerlink" title="单步更新 and 回合更新"></a>单步更新 and 回合更新</h2><p><img src="/../img/blogs/rl/Sarsa/sl2.png" alt="1720667162152"></p><h2 id="λ取值"><a href="#λ取值" class="headerlink" title="λ取值"></a>λ取值</h2><p><img src="/../img/blogs/rl/Sarsa/sl3.png" alt="1720667162152"></p><ul><li>当<code>λ=0</code>, 就变成了Sarsa的单步更新, 当<code>λ=1</code>, 就变成了回合更新, 对所有步更新的力度都是一样。 当<code>λ=(0,1)</code>之间, 取值越大, 离宝藏越近的步更新力度越大。</li></ul><h2 id="Sarsa-λ-整体算法"><a href="#Sarsa-λ-整体算法" class="headerlink" title="Sarsa(λ)整体算法"></a>Sarsa(λ)整体算法</h2><p><img src="/../img/blogs/rl/Sarsa/sl4.png" alt="sl4"></p><h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">This part of code is the Q learning brain, which is a brain of the agent.</span><br><span class="hljs-string">All decisions are made in here.</span><br><span class="hljs-string"></span><br><span class="hljs-string">View more on my tutorial page: https://morvanzhou.github.io/tutorials/</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RL</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, action_space, learning_rate=<span class="hljs-number">0.01</span>, reward_decay=<span class="hljs-number">0.9</span>, e_greedy=<span class="hljs-number">0.9</span></span>):<br>        self.actions = action_space  <span class="hljs-comment"># a list</span><br>        self.lr = learning_rate<br>        self.gamma = reward_decay<br>        self.epsilon = e_greedy<br><br>        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">check_state_exist</span>(<span class="hljs-params">self, state</span>):<br>        <span class="hljs-keyword">if</span> state <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.q_table.index:<br>            <span class="hljs-comment"># append new state to q table</span><br>            self.q_table = self.q_table.append(<br>                pd.Series(<br>                    [<span class="hljs-number">0</span>]*<span class="hljs-built_in">len</span>(self.actions),<br>                    index=self.q_table.columns,<br>                    name=state,<br>                )<br>            )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">self, observation</span>):<br>        self.check_state_exist(observation)<br>        <span class="hljs-comment"># action selection</span><br>        <span class="hljs-keyword">if</span> np.random.rand() &lt; self.epsilon:<br>            <span class="hljs-comment"># choose best action</span><br>            state_action = self.q_table.loc[observation, :]<br>            <span class="hljs-comment"># some actions may have the same value, randomly choose on in these actions</span><br>            action = np.random.choice(state_action[state_action == np.<span class="hljs-built_in">max</span>(state_action)].index)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># choose random action</span><br>            action = np.random.choice(self.actions)<br>        <span class="hljs-keyword">return</span> action<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self, *args</span>):<br>        <span class="hljs-keyword">pass</span><br><br><br><span class="hljs-comment"># backward eligibility traces</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SarsaLambdaTable</span>(<span class="hljs-title class_ inherited__">RL</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, actions, learning_rate=<span class="hljs-number">0.01</span>, reward_decay=<span class="hljs-number">0.9</span>, e_greedy=<span class="hljs-number">0.9</span>, trace_decay=<span class="hljs-number">0.9</span></span>):<br>        <span class="hljs-built_in">super</span>(SarsaLambdaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)<br><br>        <span class="hljs-comment"># backward view, eligibility trace.</span><br>        self.lambda_ = trace_decay<br>        self.eligibility_trace = self.q_table.copy()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">check_state_exist</span>(<span class="hljs-params">self, state</span>):<br>        <span class="hljs-keyword">if</span> state <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.q_table.index:<br>            <span class="hljs-comment"># append new state to q table</span><br>            to_be_append = pd.Series(<br>                    [<span class="hljs-number">0</span>] * <span class="hljs-built_in">len</span>(self.actions),<br>                    index=self.q_table.columns,<br>                    name=state,<br>                )<br>            self.q_table = self.q_table.append(to_be_append)<br><br>            <span class="hljs-comment"># also update eligibility trace</span><br>            self.eligibility_trace = self.eligibility_trace.append(to_be_append)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self, s, a, r, s_, a_</span>):<br>        self.check_state_exist(s_)<br>        q_predict = self.q_table.loc[s, a]<br>        <span class="hljs-keyword">if</span> s_ != <span class="hljs-string">&#x27;terminal&#x27;</span>:<br>            q_target = r + self.gamma * self.q_table.loc[s_, a_]  <span class="hljs-comment"># next state is not terminal</span><br>        <span class="hljs-keyword">else</span>:<br>            q_target = r  <span class="hljs-comment"># next state is terminal</span><br>        error = q_target - q_predict<br><br>        <span class="hljs-comment"># increase trace amount for visited state-action pair</span><br><br>        <span class="hljs-comment"># Method 1:</span><br>        <span class="hljs-comment"># self.eligibility_trace.loc[s, a] += 1</span><br><br>        <span class="hljs-comment"># Method 2:</span><br>        self.eligibility_trace.loc[s, :] *= <span class="hljs-number">0</span><br>        self.eligibility_trace.loc[s, a] = <span class="hljs-number">1</span><br><br>        <span class="hljs-comment"># Q update</span><br>        self.q_table += self.lr * error * self.eligibility_trace<br><br>        <span class="hljs-comment"># decay eligibility trace after update</span><br>        self.eligibility_trace *= self.gamma*self.lambda_<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>RL算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Q-Learning</title>
    <link href="/2024/07/12/rl/1-Q-Learning/"/>
    <url>/2024/07/12/rl/1-Q-Learning/</url>
    
    <content type="html"><![CDATA[<h1 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h1><h2 id="QLearning-决策"><a href="#QLearning-决策" class="headerlink" title="QLearning 决策"></a>QLearning 决策</h2><p><img src="/../../img/blogs/rl/Q-Learning/q2.png" alt="1720667078392"></p><ul><li>学习完成后，根据当前状态在Q值表中的<em><strong>最大</strong></em>Q值来选取动作</li></ul><h2 id="QLearning更新"><a href="#QLearning更新" class="headerlink" title="QLearning更新"></a>QLearning更新</h2><p><img src="/../../img/blogs/rl/Q-Learning/q3.png" alt="1720667162152"></p><ul><li><strong>更新Q值表</strong>：通过计算现实Q值和估计Q值的差距来更新</li><li><strong>现实Q值</strong>：通过<em><strong>想象</strong></em>在下个状态选择的Q值（<em><strong>max</strong></em>）乘上衰减系数，并加上到达下个状态的奖励作为现实Q值（off-policy）</li><li><strong>估计Q值</strong>：原Q值表中对应的Q值</li></ul><h2 id="QLearning整体算法"><a href="#QLearning整体算法" class="headerlink" title="QLearning整体算法"></a>QLearning整体算法</h2><p><img src="/../../img/blogs/rl/Q-Learning/q4.png" alt="1720667162152"></p><ul><li><strong>迷人之处</strong>：在<code>Q(s1, a2)</code>现实中, 也包含了一个<code>Q(s2)</code>的最大估计值，将对下一步的衰减的最大估计和当前所得到的奖励当成这一步的现实Q值。</li><li><strong>Epsilon greedy</strong>：是用在决策上的一种策略, 比如 epsilon &#x3D; 0.9 时, 就说明有90% 的情况我会按照 Q 表的最优值选择行为, 10% 的时间使用随机选行为。</li><li><strong>alpha</strong>：学习率, 来决定这次的误差有多少是要被学习的, alpha是一个小于1 的数。</li><li><strong>gamma</strong>：对未来 reward 的衰减值。</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">This part of code is the Q learning brain, which is a brain of the agent.</span><br><span class="hljs-string">All decisions are made in here.</span><br><span class="hljs-string"></span><br><span class="hljs-string">View more on my tutorial page: https://morvanzhou.github.io/tutorials/</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br>np.random.seed(<span class="hljs-number">2</span>)  <span class="hljs-comment"># reproducible</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">QLearningTable</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, actions, learning_rate=<span class="hljs-number">0.01</span>, reward_decay=<span class="hljs-number">0.9</span>, e_greedy=<span class="hljs-number">0.9</span></span>):<br>        self.actions = actions  <span class="hljs-comment"># a list</span><br>        self.lr = learning_rate<br>        self.gamma = reward_decay<br>        self.epsilon = e_greedy<br>        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_action</span>(<span class="hljs-params">self, observation</span>):<br>        self.check_state_exist(observation)<br>        <span class="hljs-comment"># action selection</span><br>        <span class="hljs-keyword">if</span> np.random.uniform() &lt; self.epsilon:<br>            <span class="hljs-comment"># choose best action</span><br>            state_action = self.q_table.loc[observation, :]<br>            <span class="hljs-comment"># some actions may have the same value, randomly choose on in these actions</span><br>            action = np.random.choice(state_action[state_action == np.<span class="hljs-built_in">max</span>(state_action)].index)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># choose random action</span><br>            action = np.random.choice(self.actions)<br>        <span class="hljs-keyword">return</span> action<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">learn</span>(<span class="hljs-params">self, s, a, r, s_</span>):<br>        self.check_state_exist(s_)<br>        q_predict = self.q_table.loc[s, a]<br>        <span class="hljs-keyword">if</span> s_ != <span class="hljs-string">&#x27;terminal&#x27;</span>:<br>            q_target = r + self.gamma * self.q_table.loc[s_, :].<span class="hljs-built_in">max</span>()  <span class="hljs-comment"># next state is not terminal</span><br>        <span class="hljs-keyword">else</span>:<br>            q_target = r  <span class="hljs-comment"># next state is terminal</span><br>        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  <span class="hljs-comment"># update</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">check_state_exist</span>(<span class="hljs-params">self, state</span>):<br>        <span class="hljs-keyword">if</span> state <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.q_table.index:<br>            <span class="hljs-comment"># append new state to q table</span><br>            self.q_table = self.q_table.append(<br>                pd.Series(<br>                    [<span class="hljs-number">0</span>]*<span class="hljs-built_in">len</span>(self.actions),<br>                    index=self.q_table.columns,<br>                    name=state,<br>                )<br>            )<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>RL算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker 学习</title>
    <link href="/2024/05/26/docker_learn_note/"/>
    <url>/2024/05/26/docker_learn_note/</url>
    
    <content type="html"><![CDATA[<h2 id="docker-学习笔记"><a href="#docker-学习笔记" class="headerlink" title="docker 学习笔记"></a>docker 学习笔记</h2><h3 id="1-基础教程"><a href="#1-基础教程" class="headerlink" title="1. 基础教程"></a>1. 基础教程</h3><p>参考教程：</p><ol><li><a href="https://blog.csdn.net/qq_40298902/article/details/105967342">菜鸟入门Docker</a></li><li><a href="https://blog.csdn.net/qq_32101863/article/details/120341856">Docker 封装anaconda环境，生成镜像并打包1</a></li><li><a href="https://blog.csdn.net/qq_32101863/article/details/120344080">Docker 封装anaconda环境，生成镜像并打包2</a></li><li><a href="https://www.runoob.com/docker/docker-image-usage.html">Docker 菜鸟教程</a></li></ol><h3 id="2-docker镜像准备"><a href="#2-docker镜像准备" class="headerlink" title="2. docker镜像准备"></a>2. docker镜像准备</h3><h4 id="下载基础镜像"><a href="#下载基础镜像" class="headerlink" title="下载基础镜像"></a>下载基础镜像</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker pull ubuntu:18.04<br></code></pre></td></tr></table></figure><h4 id="制作镜像"><a href="#制作镜像" class="headerlink" title="制作镜像"></a>制作镜像</h4><p>编写Dockerfile文件，内容示例如下：<br>    <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">使用官方的 Python 3.8 镜像作为基础镜像</span><br>FROM python:3.8<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">安装 wget</span><br><br>RUN apt-get update &amp;&amp; apt-get install -y wget &amp;&amp; rm -rf /var/lib/apt/lists/*<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">下载 Miniconda 安装脚本</span><br><br>RUN wget -q[https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh](https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh) -O miniconda.sh<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">安装 Miniconda</span><br><br>RUN bash miniconda.sh -b -p /opt/miniconda &amp;&amp;<br>rm miniconda.sh<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">初始化 Conda</span><br><br>RUN /opt/miniconda/bin/conda init bash<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">创建新的 Conda 环境</span><br><br>ENV CONDA_ENV=myenv<br>RUN conda create -y --name $CONDA_ENV python=3.8<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">激活 Conda 环境</span><br><br>RUN echo &quot;conda activate $CONDA_ENV&quot; &gt;&gt; ~/.bashrc<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">安装所需的包</span><br><br>RUN conda install -y -n $CONDA_ENV numpy pandas matplotlib<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">设置容器内的工作目录</span><br><br>WORKDIR /usr/src/app<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">设置容器启动时的命令</span><br><br>CMD [&quot;/bin/bash&quot;]<br></code></pre></td></tr></table></figure></p><h4 id="构建镜像"><a href="#构建镜像" class="headerlink" title="构建镜像"></a>构建镜像</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker build --network host -t mirrors.tencent.com/star_library/tlinux-64bit-python3.6:0415 .<br></code></pre></td></tr></table></figure><h4 id="推送镜像"><a href="#推送镜像" class="headerlink" title="推送镜像"></a>推送镜像</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">以下方括号内容为需要根据实际需求替换的字符串</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">登录至软件源docker registry，[user]为OA账号名，[token]为软件源系统分配，可以通过【快捷指令】获取</span><br>sudo docker login --username [user] --password [token] mirrors.tencent.com<br><span class="hljs-meta prompt_"> </span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">push镜像至软件源docker registry，[image_id]即docker镜像的<span class="hljs-built_in">id</span>，可以通过sudo docker image list命令查看</span><br>sudo docker tag [image_id] mirrors.tencent.com/[namespace]/[repo]:[tag]<br>sudo docker push mirrors.tencent.com/[namespace]/[repo]:[tag]<br><br>如<br>docker push mirrors.tencent.com/gethinhu/tlinux-64bit-python3.6:0415<br></code></pre></td></tr></table></figure><h3 id="3-调试"><a href="#3-调试" class="headerlink" title="3. 调试"></a>3. 调试</h3><p>为了保证调试时训练程序运行环境与线上实际的运行环境一致，需要在开发机上用与线上相同的docker镜像启动容器</p><p>为了方便，平台也准备了一个脚本run_docker.sh，可以从附件下载使用。使用方法：</p><p>注意：</p><p>如果报错‘run_docker.sh : command not found’，请检查&#x2F;usr&#x2F;local&#x2F;bin是否包含在$PATH里，如果没有，将其加入$PATH。</p><p>如果报错‘docker: Error response from daemon: could not select device driver “” with capabilities: [[gpu]].’，是因为开发机没安装nvidia-docker，可以联系O2000(Oteam统一技术支持)解决。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">1. 编写run_docker.sh</span><br>        # 示例<br><br>        #!/bin/bash<br>        image_full_name=$1<br>        if [ -z $&#123;image_full_name&#125; ]<br>        then<br>        echo &quot;mirrors.tencent.com/lllzheng/cuda11.8-ubuntu20.04:0511    \n&quot;<br>        exit 0<br>        fi<br><br>        cmd=$2<br>        if [ -z &quot;$&#123;cmd&#125;&quot; ]<br>        then<br>        echo &quot;cd /apdcephfs/private_lllzheng/taiji/peg-in-hole-6D/     \n&quot;<br>        echo &quot;conda activate mujoco     \n&quot;<br>        echo &quot;python3 /apdcephfs/private_lllzheng/taiji/peg-in-hole-6D/try_train.py     \n&quot;<br>        exit 0<br>        fi<br><br>        echo $&#123;image_full_name&#125;<br>        echo $&#123;cmd&#125;<br>        docker run -it --gpus all --network=host --name test -v /apdcephfs/:/apdcephfs/private_lllzheng  $&#123;image_full_name&#125;  $&#123;cmd&#125;<br><br>        #如果docker -v版本小于19.03，请使用以下docker run命令<br>        #docker run -it -e NVIDIA_VISIBLE_DEVICES=all --network=host -v /apdcephfs/:/apdcephfs/  $&#123;image_full_name&#125;  $&#123;cmd&#125;<br><br>        # sudo bash run_docker.sh mirrors.tencent.com/lllzheng/cuda11.8-ubuntu20.04:0511 /bin/bash -c &quot;cd /apdcephfs/private_lllzheng/taiji/peg-in-hole-6D/ &amp;&amp; conda activate mujoco &amp;&amp; python3 /apdcephfs/private_lllzheng/taiji/peg-in-hole-6D/try_train.py&quot;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">2. 运行run_docker.sh</span><br>chmod +x run_docker.sh<br>run_docker.sh $&#123;image_full_name&#125; $&#123;cmd&#125;<br>eg：run_docker.sh mirrors.tencent.com/star_library/g-tlinux2.2-python3.6-cuda9.0-cudnn7.6-tf1.12:latest /bin/bash -c &quot;python3.6 /apdcephfs/private_gethinhu/mnist_trainer/fully_connected_feed.py --train_dir /apdcephfs/share_986015/mnist_dataset/ --max_steps 100000 --batch_size=1000&quot;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mujoco-py 学习</title>
    <link href="/2024/05/26/robot-sim/mujoco-py%20learn_note/"/>
    <url>/2024/05/26/robot-sim/mujoco-py%20learn_note/</url>
    
    <content type="html"><![CDATA[<h2 id="mujoco-py-学习笔记"><a href="#mujoco-py-学习笔记" class="headerlink" title="mujoco-py 学习笔记"></a>mujoco-py 学习笔记</h2><h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install mujoco-py<br></code></pre></td></tr></table></figure><h3 id="常见安装错误"><a href="#常见安装错误" class="headerlink" title="常见安装错误"></a>常见安装错误</h3><ul><li>如果报错 Cython.Compiler.Errors.CompileError:</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip uninstall cython<br>pip install cython==0.29.21<br></code></pre></td></tr></table></figure><ul><li>tensorboard打不开</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 第一步：查看端口占用：</span><br>lsof -i:6006<br><span class="hljs-comment"># 第二步：关掉占用端口的进程</span><br><span class="hljs-built_in">kill</span> -9 PID<br></code></pre></td></tr></table></figure><h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p><a href="https://blog.csdn.net/weixin_44420419/article/details/116231500">mujoco-py安装遇问题</a><br><a href="https://blog.csdn.net/weixin_44420419/article/details/116519279">mujoco-py渲染问题</a></p><h3 id="小记"><a href="#小记" class="headerlink" title="小记"></a>小记</h3><h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> mujoco_py<br>model = mujoco_py.load_model_from_path(<span class="hljs-string">&quot;path/to/model.xml&quot;</span>)<br>sim = mujoco_py.MjSim(model)<br></code></pre></td></tr></table></figure><p>创建了一个 MjSim 对象，它表示一个 MuJoCo 仿真实例。通过这个对象，执行各种操作，例如执行仿真步骤、获取和设置模型状态、渲染仿真环境等。使用 sim 对象实现或获取的操作：</p><ul><li>执行仿真步骤：使用 <code>sim.step()</code>方法，执行一步仿真。这将根据当前的模型状态和控制输入更新模型的状态。</li><li>获取和设置模型状态：使用 <code>sim.data</code>属性来获取和设置模型的状态。例如，获取关节角度、速度、力矩等信息，也可以设置关节力矩、外部力等。</li><li>渲染仿真环境：使用 <code>sim.render()</code>方法，渲染仿真环境。这将在屏幕上显示仿真环境的当前状态。</li><li>获取和设置控制输入：使用 <code>sim.data.ctrl</code>属性来获取和设置控制输入。例如，设置关节的目标位置、速度或力矩等。</li><li>获取和设置模型参数：使用 <code>sim.model</code>属性来获取和设置模型参数。例如，获取关节的惯量、刚度等信息，也可以设置关节的摩擦系数、阻尼系数等。</li><li>保存和加载仿真状态：使用 <code>sim.save()</code>和 <code>sim.load()</code>方法来保存和加载仿真状态。这可以用于暂停和恢复仿真，或者在多个仿真实例之间共享状态。</li><li>获取和设置相机参数：使用 <code>sim.render_context</code>属性来获取和设置相机参数。例如，设置相机的位置、方向、视野角等。</li></ul><h5 id="sim-data中一些主要的属性"><a href="#sim-data中一些主要的属性" class="headerlink" title="sim.data中一些主要的属性"></a>sim.data中一些主要的属性</h5><ul><li>qpos：关节位置的数组，这个数组包含了模拟中所有关节的位置和所有自由物体的位置和方向。</li><li>qvel：关节速度的数组。</li><li>qacc：关节加速度的数组。</li><li>qfrc_applied：施加到每个关节的外部力的数组。</li><li>qfrc_actuator:</li><li>ctrl：控制器的控制信号的数组。</li><li>time：模拟的当前时间。</li><li>xpos：模型中每个物体的位置的数组。</li><li>xquat：模型中每个物体的四元数表示的方向的数组。</li><li>xmat：模型中每个物体的旋转矩阵表示的方向的数组。</li><li>xipos：模型中每个惯性元素的位置的数组。</li><li>ximat：模型中每个惯性元素的旋转矩阵表示的方向的数组。</li><li>contact：模型中的接触信息。</li></ul><h5 id="PyMjData"><a href="#PyMjData" class="headerlink" title="PyMjData"></a>PyMjData</h5><ul><li><p><code>act</code>: 执行器激活数组，用于控制执行器的激活状态。</p></li><li><p><code>act_dot</code>: 执行器激活的变化率。</p></li><li><p><code>active_contacts_efc_pos</code>: 活动接触点的全局位置。</p></li><li><p><code>actuator_force</code>: 执行器产生的力。</p></li><li><p><code>actuator_length</code>: 执行器的长度。</p></li><li><p><code>actuator_moment</code>: 执行器产生的力矩。</p></li><li><p><code>actuator_velocity</code>: 执行器的速度。</p></li><li><p><code>body_jacp, body_jacr</code>: 与身体相关的雅可比矩阵的偏导数。</p></li><li><p><code>body_xmat, body_xpos, body_xquat, body_xvelp, body_xvelr</code>: 描述身体的位置、方向、速度和旋转的矩阵和向量。</p></li><li><p><code>cacc</code>: 接触点的累积加速度。</p></li><li><p><code>cam_xmat, cam_xpos</code>: 相机的位置和方向。</p></li><li><p><code>cdof, cdof_dot</code>: 自由度的当前值和变化率。</p></li><li><p><code>cfrc_ext</code>: 外部力。</p></li><li><p><code>cfrc_int</code>: 内部力。</p></li><li><p><code>cinert</code>: 惯性张量。</p></li><li><p><code>contact</code>: 接触信息。</p></li><li><p><code>crb</code>: 约束雅可比矩阵。</p></li><li><p><code>ctrl</code>: 控制输入。</p></li><li><p><code>cvel</code>: 接触点的速度。</p></li><li><p><code>efc_AR, efc_D, efc_J, efc_R</code>: 接触力雅可比矩阵、阻尼矩阵、约束雅可比矩阵和旋转矩阵。</p></li><li><p><code>energy</code>: 系统能量。</p></li><li><p><code>geom_jacp, geom_jacr, geom_xmat, geom_xpos, geom_xvelp, geom_xvelr</code>: 几何体的位置、方向、速度。</p></li><li><p><code>light_xdir, light_xpos</code>: 光源的方向和位置。</p></li><li><p><code>maxuse_con, maxuse_efc, maxuse_stack</code>: 各种数组的最大使用数量。</p></li><li><p><code>mocap_pos, mocap_quat</code>: 运动捕捉的位置和四元数。</p></li><li><p><code>nbuffer, ncon, ne, nefc, nf, nstack</code>: 不同类型数据的数量。</p></li><li><p><code>pstack</code>: 预测栈。</p></li><li><p><code>qLD, qLDiagInv, qLDiagSqrtInvInv, qM, qacc, qacc_unc, qacc_warmstart</code>: 与线性代数相关的矩阵和向量。</p></li><li><p><code>qfrc_actuator, qfrc_applied, qfrc_bias, qfrc_constraint, qfrc_inverse, qfrc_passive, qfrc_unc</code>: 与力相关的向量。</p></li><li><p><code>qpos, qvel</code>: 关节位置和速度状态。</p></li><li><p><code>sensordata</code>: 传感器数据。</p></li><li><p><code>set_joint_qpos, set_joint_qvel</code>: 设置关节位置和速度的方法。</p></li><li><p><code>set_mocap_pos, set_mocap_quat</code>: 设置运动捕捉位置和四元数的方法。</p></li><li><p><code>site_jacp, site_jacr, site_xmat, site_xpos, site_xvelp, site_xvelr</code>: 站点的雅可比、位置、方向、速度。</p></li><li><p><code>solver, solver_fwdinv, solver_iter, solver_nnz</code>: 与求解器相关的属性。</p></li><li><p><code>subtree_angmom, subtree_com, subtree_linvel</code>: 子树的角动量、质心和线性速度。</p></li><li><p><code>ten_length, ten_moment, ten_velocity, ten_wrapadr, ten_wrapnum</code>: 肌腱的长度、力矩、速度和缠绕信息。</p></li><li><p><code>time, timer</code>: 仿真时间。</p></li><li><p><code>userdata</code>: 用户自定义数据。</p></li><li><p><code>warning</code>: 警告信息。</p></li><li><p><code>wrap_obj, wrap_xpos</code>: 与肌腱缠绕相关的对象和位置。</p></li><li><p><code>xanchor, xaxis</code>: 与接触有关的锚点和轴。</p></li><li><p><code>ximat, xpos</code>: 位置和方向。</p></li><li><p><code>xfrc_applied</code>: 应用的外部力。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>Mujoco</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Mujoco</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Uubuntu 系统配置</title>
    <link href="/2024/05/25/ubuntu%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE/"/>
    <url>/2024/05/25/ubuntu%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="一、-安装浏览器"><a href="#一、-安装浏览器" class="headerlink" title="一、 安装浏览器"></a>一、 安装浏览器</h2><ul><li><p>安装Edge浏览器</p></li><li><p>安装网络工具<br>工具在手，天下我有！<br><a href="https://github.com/clash-verge-rev/clash-verge-rev/releases/">下载地址</a></p></li><li><p>VsCode安装和更新</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 安装</span><br>wget http://fishros.com/install -O fishros &amp;&amp; . fishros<br><br><span class="hljs-comment">#先更新源</span><br>sudo apt-get update<br><br><span class="hljs-comment">#再安装vscode</span><br>sudo apt-get install code<br></code></pre></td></tr></table></figure></li></ul><h2 id="二、配置终端"><a href="#二、配置终端" class="headerlink" title="二、配置终端"></a>二、配置终端</h2><p><a href="https://blog.csdn.net/qq_43447339/article/details/135758451">zsh配置</a></p><h2 id="三、软件、环境等配置"><a href="#三、软件、环境等配置" class="headerlink" title="三、软件、环境等配置"></a>三、软件、环境等配置</h2><h3 id="1-ROS2配置"><a href="#1-ROS2配置" class="headerlink" title="1. ROS2配置"></a>1. ROS2配置</h3><p><a href="https://fishros.org.cn/forum/topic/20/%E5%B0%8F%E9%B1%BC%E7%9A%84%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85%E7%B3%BB%E5%88%97?lang=zh-CN">ROS2一键安装教程</a></p><h3 id="2-深度学习环境配置"><a href="#2-深度学习环境配置" class="headerlink" title="2. 深度学习环境配置"></a>2. 深度学习环境配置</h3><h4 id="1-CUDA安装"><a href="#1-CUDA安装" class="headerlink" title="(1) CUDA安装"></a>(1) CUDA安装</h4><ul><li>软件更新—&gt;附加驱动</li><li>ubuntu 20.04 cuda12.0安装命令<br><a href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA下载地址</a><br><a href="https://blog.csdn.net/weixin_37926734/article/details/123033286">参考博客</a></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin<br>sudo <span class="hljs-built_in">mv</span> cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600<br>wget https://developer.download.nvidia.com/compute/cuda/12.0.0/local_installers/cuda-repo-ubuntu2004-12-0-local_12.0.0-525.60.13-1_amd64.deb<br>sudo dpkg -i cuda-repo-ubuntu2004-12-0-local_12.0.0-525.60.13-1_amd64.deb<br>sudo <span class="hljs-built_in">cp</span> /var/cuda-repo-ubuntu2004-12-0-<span class="hljs-built_in">local</span>/cuda-*-keyring.gpg /usr/share/keyrings/<br>sudo apt-get update<br>sudo apt-get -y install cuda<br></code></pre></td></tr></table></figure><h4 id="2-Conda环境配置"><a href="#2-Conda环境配置" class="headerlink" title="(2) Conda环境配置"></a>(2) Conda环境配置</h4><ul><li>关闭自动激活的base环境</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda config --<span class="hljs-built_in">set</span> auto_activate_base <span class="hljs-literal">false</span><br></code></pre></td></tr></table></figure><h4 id="3-Git配置"><a href="#3-Git配置" class="headerlink" title="(3) Git配置"></a>(3) Git配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">git安装与卸载<br>apt-get install git apt-get remove git<br><br>git配置<br>配置用户名<br>git config --global user.name “zl”<br>配置邮箱<br>git config --global user.email “zlsy@mail.ustc.edu.cn”<br>查看配置信息<br>git config --global --list<br>生成公钥<br>ssh-keygen -t rsa -C <span class="hljs-string">&quot;zlsy@mail.ustc.edu.cn&quot;</span><br></code></pre></td></tr></table></figure><h4 id="4-pip镜像源配置"><a href="#4-pip镜像源配置" class="headerlink" title="(4) pip镜像源配置"></a>(4) pip镜像源配置</h4><ul><li>临时使用pip镜像源</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo pip install numpy -i https://pypi.tuna.tsinghua.edu.cn/simple/ <br></code></pre></td></tr></table></figure><ul><li>永久配置</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> ~/.pip  <span class="hljs-comment">#创建一个名为.pip的文件夹</span><br><span class="hljs-built_in">cd</span> ~/.pip    <span class="hljs-comment">#进入创建的文件夹</span><br><span class="hljs-built_in">touch</span> pip.conf  <span class="hljs-comment">#创建pip.conf</span><br>sudo gedit ~/.pip/pip.conf   <span class="hljs-comment">#编辑文件</span><br> <br>复制下面的内容到文件中（配置的豆瓣源，也可以配置别的）<br>[global]<br>index-url = https://pypi.mirrors.ustc.edu.cn/simple/<br>[install]<br>trusted-host=pypi.mirrors.ustc.edu.cn<br><br>镜像源推荐<br>（1）阿里云 https://mirrors.aliyun.com/pypi/simple/<br>（2）豆瓣https://pypi.douban.com/simple/<br>（3）清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/<br>（4）中国科学技术大学 https://pypi.mirrors.ustc.edu.cn/simple/<br>（5）华中科技大学https://pypi.hustunique.com/<br></code></pre></td></tr></table></figure><h3 id="3-软件安装"><a href="#3-软件安装" class="headerlink" title="3.软件安装"></a>3.软件安装</h3><h4 id="CoppeliaSim"><a href="#CoppeliaSim" class="headerlink" title="CoppeliaSim"></a>CoppeliaSim</h4><ul><li><a href="https://blog.csdn.net/qq_43447339/article/details/135889186">创建快捷方式</a></li><li>终端打开（推荐）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">alias</span> vrep=<span class="hljs-string">&#x27;/opt/CoppeliaSim/CoppeliaSim_Edu_V4_1_0_Ubuntu20_04/coppeliaSim.sh&#x27;</span><br></code></pre></td></tr></table></figure><h4 id="Mujoco"><a href="#Mujoco" class="headerlink" title="Mujoco"></a>Mujoco</h4><ul><li><a href="https://blog.csdn.net/weixin_51844581/article/details/128454472">Ubuntu20.04安装Mujoco</a></li></ul><h2 id="四、相关问题"><a href="#四、相关问题" class="headerlink" title="四、相关问题"></a>四、相关问题</h2><h3 id="1-ubuntu与windows双系统安装，缺少启动引导项"><a href="#1-ubuntu与windows双系统安装，缺少启动引导项" class="headerlink" title="1. ubuntu与windows双系统安装，缺少启动引导项"></a>1. <a href="https://blog.csdn.net/qq_43447339/article/details/146441068?spm=1001.2014.3001.5501">ubuntu与windows双系统安装，缺少启动引导项</a></h3><p>  —————————-完善中——————————-</p>]]></content>
    
    
    <categories>
      
      <category>OS</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Ubuntu</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mujoco Q&amp;A</title>
    <link href="/2024/05/25/robot-sim/mujoco_question/"/>
    <url>/2024/05/25/robot-sim/mujoco_question/</url>
    
    <content type="html"><![CDATA[<h2 id="一、Mujoco安装"><a href="#一、Mujoco安装" class="headerlink" title="一、Mujoco安装"></a>一、Mujoco安装</h2><p>这是一句话<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Mujoco Python](https://mujoco.readthedocs.io/en/stable/python.html#)">[1]</span></a></sup></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://mujoco.readthedocs.io/en/stable/python.html#">Mujoco Python</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Mujoco</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Mujoco</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mujoco 学习笔记</title>
    <link href="/2024/05/25/robot-sim/mujoco_learn/"/>
    <url>/2024/05/25/robot-sim/mujoco_learn/</url>
    
    <content type="html"><![CDATA[<h2 id="一、Mujoco安装"><a href="#一、Mujoco安装" class="headerlink" title="一、Mujoco安装"></a>一、Mujoco安装</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pip install mujoco<br></code></pre></td></tr></table></figure><h2 id="二、交互式查看器"><a href="#二、交互式查看器" class="headerlink" title="二、交互式查看器"></a>二、交互式查看器</h2><h3 id="命令行打开"><a href="#命令行打开" class="headerlink" title="命令行打开"></a>命令行打开</h3><ul><li>启动一个空的可视化会话，其中可以通过拖放加载模型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python -m mujoco.viewer<br></code></pre></td></tr></table></figure><ul><li>指定加载模型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python -m mujoco.viewer --mjcf=/path/to/some/mjcf.xml<br></code></pre></td></tr></table></figure><h3 id="Python程序中打开"><a href="#Python程序中打开" class="headerlink" title="Python程序中打开"></a>Python程序中打开</h3><ul><li><code>viewer.launch()</code></li></ul><p>启动一个空的可视化会话，其中可以通过拖放加载模型。</p><ul><li><code>viewer.launch(model)</code></li></ul><p>可视化工具启动给定的可视化会话 在内部创建自己的实例</p><ul><li><code>viewer.launch(model, data)</code></li></ul><p>与上述相同，只是可视化工具直接在给定实例上运行，退出时对象将被修改。</p><h3 id="被动查看器"><a href="#被动查看器" class="headerlink" title="被动查看器"></a>被动查看器</h3><ul><li><code>viewer.launch_passive(model, data)</code></li></ul><p>创建一个被动查看器实例。在此模式下，函数不会阻塞，允许代码继续执行。用户的脚本负责计时和推进物理状态，除非用户显式同步传入事件，否则鼠标拖动将不起作用</p><h2 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> mujoco<br></code></pre></td></tr></table></figure><h2 id="参考-1"><a href="#参考-1" class="headerlink" title="参考[1]"></a>参考<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Mujoco Python](https://mujoco.readthedocs.io/en/stable/python.html#)">[1]</span></a></sup></h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://mujoco.readthedocs.io/en/stable/python.html#">Mujoco Python</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Mujoco</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Mujoco</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/05/25/hello-world/"/>
    <url>/2024/05/25/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Windows/Ubuntu22.04 解决CoppeliaSim界面字体过小问题</title>
    <link href="/2024/05/25/robot-sim/coppeliasim/"/>
    <url>/2024/05/25/robot-sim/coppeliasim/</url>
    
    <content type="html"><![CDATA[<h2 id="Ubuntu22-04-解决VREP-CoppeliaSim-界面字体过小问题"><a href="#Ubuntu22-04-解决VREP-CoppeliaSim-界面字体过小问题" class="headerlink" title="Ubuntu22.04 解决VREP&#x2F;CoppeliaSim 界面字体过小问题"></a>Ubuntu22.04 解决VREP&#x2F;CoppeliaSim 界面字体过小问题</h2><h3 id="Windows10"><a href="#Windows10" class="headerlink" title="Windows10"></a>Windows10</h3><p>1.右键单击CoppeliaSim.exe（可能的位置：C：\ Program Files \ CoppeliaRobotics \ CoppeliaSimEdu），然后单击“属性”。</p><p>2.在“兼容性”选项卡下</p><p>3.单击“更改高DPI设置”</p><p>4.选中“替代高DPI缩放行为”框</p><p>5.缩放比例–选择“系统”</p><h3 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu"></a>Ubuntu</h3><p>1.找到CoppeliaSim安装位置（例如：&#x2F;opt&#x2F;CoppeliaSimo_Edu_V4_2_0_Ubuntu20_04）<br>2.打开system 文件夹下usrset.txt文件，将变量“ highhandedness”的值更改为2<br>3.重启CoppeliaSim<br><img src="/../../img/blogs/coppeliasim.png" alt="image"></p>]]></content>
    
    
    <categories>
      
      <category>CoppeliaSim</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Coppeliasim</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
